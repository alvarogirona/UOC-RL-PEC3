{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATAcq7bQyNxe"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"https://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/llibre-estil/logo-UOC-masterbrand-vertical.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.883 · Aprendizaje por refuerzo</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2023-1 · Máster universitario en Ciencia de datos (<i>Data science</i>)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-bottom: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "# PEC2: Deep Reinforcement Learning\n",
    "\n",
    "\n",
    "En esta práctica se implementarán - modelos de DRL en dos entornos diferentes, con el objetivo de analizar distintas formas de aprendizaje de un agente y estudiar su rendimiento. El agente será entrenado con los métodos:\n",
    "\n",
    "<ol>\n",
    "    <li>DQN</li>\n",
    "    <li>Dueling DQN</li>\n",
    "</ol>\n",
    "  \n",
    "\n",
    "**Importante: La entrega debe hacerse en formato notebook y en formato html donde se vea el código y los resultados y comentarios de cada ejercicio. Para exportar el notebook a html puede hacerse desde el menú File  →  Download as  →  HTML.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZXQSvJSyNxk"
   },
   "source": [
    "## 0. Contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zan-tCv_zJXF"
   },
   "source": [
    "El aprendizaje por refuerzo es un campo de la inteligencia artificial que busca desarrollar sistemas capaces de aprender y tomar decisiones autónomas a través de la interacción con su entorno. A lo largo de los años, este enfoque ha demostrado su capacidad para abordar una amplia gama de aplicaciones, desde juegos de mesa hasta robótica y gestión de recursos. Sin embargo, una de las cuestiones más desafiantes en el aprendizaje por refuerzo es la creación de entornos de simulación adecuados que reflejen fielmente el contexto de la aplicación deseada.\n",
    "\n",
    "En este contexto, esta PEC tiene como objetivo desarrollar un nuevo entorno de simulación que permita la investigación y experimentación con diferentes agentes de trading. Este entorno estará diseñado específicamente para abordar un problema ficticio de inversión y gestión de un portafolio en el mercado de valores, en el que un agente debe aprender a tomar decisiones óptimas de compra, venta o mantenimiento de acciones. El objetivo del agente será maximizar las ganancias a lo largo del tiempo mediante estrategias basadas en el aprendizaje por refuerzo.\n",
    "\n",
    "Para ello, se utilizará el entorno adaptado a las especificaciones de Gymnasium (https://gymnasium.farama.org/index.html), que permite la creación de entornos personalizados para el aprendizaje por refuerzo. Este entorno simulará el comportamiento dinámico de un mercado financiero, con fluctuaciones en los precios de las acciones y eventos de mercado que afecten las decisiones del agente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzjteFjb1JS1"
   },
   "source": [
    "## 1. Creación de un entorno en Gym (3 ptos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1J3ZQQ2zJXh"
   },
   "source": [
    "En este ejercicio diseñaremos un entorno sencillo siguiendo el esquema de los entornos de <code>Gymnasium</code>, y trataremos de resolverlo.\n",
    "\n",
    "Los entornos de <code>Gymnasium</code> suelen tener la siguiente estructura:\n",
    "\n",
    "```\n",
    "class FooEnv(gym.Env):\n",
    "  metadata = {'render.modes': ['human']}\n",
    "\n",
    "  def __init__(self):\n",
    "    ...\n",
    "  def step(self, action):\n",
    "    ...\n",
    "    return new_state, reward, terminated, truncated, info\n",
    "\n",
    "  def reset(self):\n",
    "    ...\n",
    "    return observation, info\n",
    "\n",
    "  def render(self, mode='human', close=False):\n",
    "    ...\n",
    "\n",
    " ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U1O0HWGzJXh"
   },
   "source": [
    "El primer paso será instalar las librerías necesarias para abordar la PEC:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TYp6OAfE1JS2",
    "ExecuteTime": {
     "end_time": "2024-12-07T15:19:45.474252Z",
     "start_time": "2024-12-07T15:19:45.472274Z"
    }
   },
   "source": [
    "# !pip install gymnasium\n",
    "# !pip install torch\n",
    "# \n",
    "# !pip install matplotlib\n",
    "# !pip install numpy\n",
    "# !pip install tensorboard\n",
    "# !pip install tdqm\n",
    "# !pip install tabulate\n",
    "# !pip install yfinance\n",
    "# !pip install pandas"
   ],
   "outputs": [],
   "execution_count": 598
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsmRtf2Z1JS3"
   },
   "source": [
    "y las importamos:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yuHxDz3ZXawz",
    "ExecuteTime": {
     "end_time": "2024-12-07T15:19:45.507866Z",
     "start_time": "2024-12-07T15:19:45.504375Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "from collections import namedtuple, deque\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "print(\"Gym Version:\", gym.__version__)  # 0.28.1\n",
    "print(\"Gym Version:\", torch.__version__)  # 0.28.1"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym Version: 1.0.0\n",
      "Gym Version: 2.2.2\n"
     ]
    }
   ],
   "execution_count": 599
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVVI0tZnp9v2"
   },
   "source": [
    "\n",
    "###  1.1 Entorno de Simulación para Trading Automático en el Mercado de Valores\n",
    "\n",
    "***¡Enhorabuena!*** Una firma de inversión ha decidido contrataros para desarrollar un sistema de trading automático para sus operaciones en el mercado de valores. Para ello, os piden que diseñéis un entorno de simulación que permita entrenar un agente capaz de tomar decisiones de compra, venta o mantener posiciones sobre una acción determinada, maximizando las ganancias a lo largo del tiempo. El entorno debe cumplir las siguientes especificaciones:\n",
    "\n",
    "<ul>\n",
    "  <li>El entorno debe llamarse <code>StockMarketEnv</code>\n",
    "  </li>\n",
    "  <li>El entorno debe heredar de la clase <code>gym.Env</code>\n",
    "  </li>\n",
    "  <li>El precio inicial de la acción estará basado en datos históricos, obtenidos a partir de una consulta a Yahoo Finance.</li>\n",
    "  <li>El balance inicial del agente será de 10,000 dolares, el cual puede ser utilizado para comprar acciones.</li>\n",
    "  <li>El agente puede realizar las siguientes acciones: </li>\n",
    "  \n",
    "  <ul>\n",
    "    <li>0 -> Mantener (no se realizan operaciones)</li>\n",
    "    <li>1 -> Comprar (se compra todas las acciones posibles al precio actual)</li>\n",
    "    <li>2 -> Vender (se vende todas las acciones disponibles al precio actual)</li>\n",
    "  </ul>\n",
    "  <li>El sistema de recompensas será el siguiente:</li>\n",
    "  <ul>\n",
    "    <li>El agente recibe una recompensa +1 si el valor neto de su portafolio (balance_actual + balance_anterior) aumenta respecto al paso anterior.</li>\n",
    "    <li>El agente recibe una recompensa de +1 si el valor neto de su portafolio (balance_actual) se mantiene igual, no posee ninguna acción y el valor de las acciones disminuye. Dicha comprobación no se realiza el primer dia de trading.</li>\n",
    "    <li>El agente recibe una recompensa de -1 si el valor neto de su portafolio (balance_actual) se mantiene igual, no posee ninguna acción y el valor de las acciones aumenta con respecto al día anterior. Dicha comprobación no se realiza el primer dia de trading. </li>\n",
    "    <li>Si el valor neto disminuye con respecto al día anterior, el agente recibe una recompensa -1.</li>\n",
    "    <li>En otros casos recibe una puntuación de 0.</li>\n",
    "  </ul>\n",
    "  <li>El entorno tendrá una duración por defecto para el entrenamiento 2019-01-01 hasta 2021-01-01 para el entrenamiento.</li>\n",
    "  <li>El entorno finalizará si el valor neto del portafolio cae por debajo del 85% del balance inicial (es decir, 8,500 dolares ).</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "El objetivo de este entorno es que el agente aprenda a tomar decisiones óptimas de compra y venta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EulU8U8F1JS4"
   },
   "source": [
    "\n",
    "![Imagen de Stock Market](https://media1.tenor.com/m/wWvt6qEQB8EAAAAd/kah.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWhwescU1JS4"
   },
   "source": [
    "#### 1.1.1 Implementación de los indicadores económicos\n",
    "\n",
    "El primer paso es implementar dos funciones llamadas `calculate_rsi` y `calculate_ema` que calcularán diferentes indicadores técnicos utilizados en el análisis de mercados financieros. Estos indicadores ayudarán a los agentes de trading a tomar decisiones basadas en patrones y tendencias del mercado.\n",
    "\n",
    "A continuación, se explica en qué consisten estas métricas:\n",
    "\n",
    "*  RSI (Índice de Fuerza Relativa): Calcula el RSI utilizando el cambio de precio durante una ventana de tiempo especificada. Este indicador muestra si un activo está sobrecomprado o sobrevendido. Podéis ver una explicación más detallada en https://es.wikipedia.org/wiki/%C3%8Dndice_de_fuerza_relativa\n",
    "*  EMA (Media Móvil Exponencial): Calcula la EMA, que es una versión ponderada de la media móvil que da más peso a los precios recientes. Podéis ver una explicación más detallada en https://es.tradingview.com/support/solutions/43000592270/\n",
    "\n",
    "Las funciones toman algunos de estos argumentos:\n",
    "\n",
    "* data: Los datos históricos de precios de las acciones, generalmente en formato de series temporales. En este ejemplo utilizaremos los precios de cierre diarios.\n",
    "* window (opcional): El número de periodos a utilizar para los cálculos de indicadores. Por defecto, se asume un valor de 14 para el RSI y la EMA.\n",
    "\n",
    "A continuación, se muestran las funciónes:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KxrX5J9U1JS4",
    "ExecuteTime": {
     "end_time": "2024-12-07T15:19:45.524976Z",
     "start_time": "2024-12-07T15:19:45.521994Z"
    }
   },
   "source": [
    "def calculate_rsi(data, window=14):\n",
    "    delta = data.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi.fillna(50).squeeze()\n",
    "\n",
    "def calculate_ema(data, window=14):\n",
    "    return data.ewm(span=window, adjust=False).mean().squeeze()\n"
   ],
   "outputs": [],
   "execution_count": 600
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfJKeOH61JS5"
   },
   "source": [
    "Con estas funciones, los agentes podrán utilizar información técnica clave sobre las acciones en el mercado para tomar decisiones de trading más informadas.\n",
    "\n",
    "Ahora bien, necesitamos el valor de las acciones. Para este proyecto, utilizamos la librería yfinance, que permite la obtención de datos históricos de activos financieros de manera sencilla. El siguiente fragmento de código descarga los datos del ETF SPY (un fondo que sigue al índice S&P 500) desde el 1 de enero de 2021 hasta el 1 de enero de 2022:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zviO1kyU1JS5",
    "ExecuteTime": {
     "end_time": "2024-12-07T15:19:45.573904Z",
     "start_time": "2024-12-07T15:19:45.552755Z"
    }
   },
   "source": [
    "data = yf.download(\"SPY\", start=\"2021-01-01\", end=\"2022-01-01\")\n",
    "\n",
    "print(data)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price      Adj Close     Close      High       Low      Open     Volume\n",
      "Ticker           SPY       SPY       SPY       SPY       SPY        SPY\n",
      "Date                                                                   \n",
      "2021-01-04 349.47168 368.79001 375.45001 364.82001 375.31000  110210800\n",
      "2021-01-05 351.87863 371.32999 372.50000 368.04999 368.10001   66426200\n",
      "2021-01-06 353.98227 373.54999 376.98001 369.12000 369.70999  107997700\n",
      "2021-01-07 359.24161 379.10001 379.89999 375.91000 376.10001   68766800\n",
      "2021-01-08 361.28845 381.26001 381.48999 377.10001 380.59000   71677200\n",
      "...              ...       ...       ...       ...       ...        ...\n",
      "2021-12-27 458.28818 477.26001 477.31000 472.01001 472.06000   56808600\n",
      "2021-12-28 457.91370 476.87000 478.81000 476.06000 477.72000   47274600\n",
      "2021-12-29 458.49945 477.48001 478.56000 475.92001 476.98001   54503000\n",
      "2021-12-30 457.23190 476.16000 479.00000 475.67001 477.92999   55329000\n",
      "2021-12-31 456.07959 474.95999 476.85999 474.67001 475.64001   65237400\n",
      "\n",
      "[252 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 601
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agW2zE6Xp9v4"
   },
   "source": [
    "El resultado es un DataFrame que contiene la siguiente información para cada día del rango de fechas:\n",
    "\n",
    "- **Open**: Precio de apertura del activo.\n",
    "- **High**: Precio máximo del activo en el día.\n",
    "- **Low**: Precio mínimo del activo en el día.\n",
    "- **Close**: Precio de cierre del activo.\n",
    "- **Adj Close**: Precio ajustado que tiene en cuenta dividendos y splits.\n",
    "- **Volume**: Número de acciones negociadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxDFUMS2p9v5"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Ejercicio 1 (0.25 ptos):</strong>\n",
    "    Utiliza los datos históricos del mercado financiero, descargados mediante la función <code>yfinance.download</code>, y aplica los indicadores técnicos proporcionados: <code>calculate_rsi</code> y  <code>calculate_ema</code>\n",
    "    A continuación, realiza las siguientes tareas:\n",
    "    <ul>\n",
    "        <li>Descarga los datos históricos de SPY para el año 2021.</li>\n",
    "        <li>Calcula el RSI para los precios de cierre durante el período.</li>\n",
    "        <li>Calcula la media móvil exponencial (EMA) para el mismo período  con los precios de cierre.</li>\n",
    "        <li>Imprime el último valor de los cálculos de cada indicador (RSI y EMA ) para verificar que se han generado correctamente sin errores. Los valores obtenidos deberían ser RSI: 53.765164 i EMA: 470.690088 (el número de decimales puede variar).</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gjYAIycip9v5",
    "ExecuteTime": {
     "end_time": "2024-12-07T15:19:45.654869Z",
     "start_time": "2024-12-07T15:19:45.632062Z"
    }
   },
   "source": [
    "data_2021 = yf.download(\"SPY\", start=\"2021-01-01\", end=\"2022-01-01\")\n",
    "rsi_2021_close = calculate_rsi(data_2021[\"Close\"])\n",
    "ema_2021_close = calculate_ema(data_2021[\"Close\"])\n",
    "\n",
    "print(\"RSI 2021 close:\")\n",
    "print(rsi_2021_close[-1])\n",
    "print(\"\\nEMA 2021 close:\")\n",
    "print(ema_2021_close[-1])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSI 2021 close:\n",
      "53.76516415158352\n",
      "\n",
      "EMA 2021 close:\n",
      "470.6900882953319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 602
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLrYAbIBzJXi"
   },
   "source": [
    "#### 1.1.2 Implementación de StockMarketEnv\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Ejercicio 2 (2.25 ptos):</strong> Define el entorno <code>StockMarketEnv</code> siguiendo las indicaciones aportadas anteriormente. Además, a parte de las típicas funciones de cualquier entorno (<code>reset</code>, <code>step</code> y <code>render</code>), deben implementarse dos funciones más (<code>save_to_csv_file</code> y <code>_normalize</code>) que se explican a continuación:\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3IO8btTNMg2"
   },
   "source": [
    "##### Crear la función <code>save_to_csv_file</code> en Python\n",
    "Además, implementa una función <code>save_to_csv_file</code> que guarde los datos actuales de una clase en un archivo CSV. La función calculará el beneficio (profit) como la diferencia entre el valor neto actual (net_worth-> balance efectivo + valor acciones en posesión) y el balance inicial (initial_balance), y escribirá una nueva fila con los valores de current_step, balance, shares_held, net_worth y profit en el archivo CSV.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:19:45.723917Z",
     "start_time": "2024-12-07T15:19:45.721354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_to_csv_file():\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": 603
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Crear la función de normalización en Python\n",
    "\n",
    "La función `_normalize` se utiliza para ajustar un valor a un rango estándar, comúnmente entre 0 y 1, en relación a un valor mínimo y máximo especificados. Este proceso, conocido como **normalización**, es útil para transformar datos, manteniendo sus proporciones relativas, dentro de un intervalo más manejable. La función toma tres parámetros:\n",
    "\n",
    "- `value`: el valor a normalizar.\n",
    "- `min_val`: el límite inferior del rango de normalización.\n",
    "- `max_val`: el límite superior del rango de normalización.\n",
    "\n",
    "La normalización se realiza mediante la siguiente fórmula:\n",
    "\n",
    "$$\n",
    "\\text{normalized_value} = \\frac{\\text{value} - \\text{min_val}}{\\text{max_val} - \\text{min_val}}\n",
    "$$\n",
    "\n",
    "\n",
    "Este cálculo ajusta `value` al rango definido entre `min_val` y `max_val`. Además, si `max_val` y `min_val` son iguales, la función devuelve `0` para evitar una **división por cero**.\n",
    "\n",
    "##### Beneficios en el Contexto de Aprendizaje por Refuerzo (RL)\n",
    "\n",
    "En un entorno de **aprendizaje por refuerzo (RL)**, la normalización es fundamental por varias razones:\n",
    "\n",
    "1. **Estabilidad de Entrenamiento**: Al normalizar las recompensas, las observaciones o las acciones a un rango estándar, se evita que valores grandes desestabilicen el proceso de aprendizaje. Modelos de RL, como las redes neuronales, tienden a aprender mejor con datos en intervalos limitados.\n",
    "\n",
    "2. **Facilita la Comparación**: Normalizar permite comparar datos provenientes de distintas fuentes o escalas, como recompensas de diferentes entornos, lo cual mejora la generalización del modelo.\n",
    "\n",
    "3. **Acelera la Convergencia**: Datos escalados de manera uniforme ayudan a que los algoritmos de RL converjan más rápidamente, ya que se reduce la variabilidad de las entradas.\n",
    "\n",
    "4. **Previene Errores de Cálculo**: Al manejar entradas normalizadas y con límites definidos, se evitan errores de cálculo o inestabilidades debidas a diferencias numéricas extremas.\n",
    "\n",
    "------\n",
    "<b>Nota</b>: se os proporciona el código pre-implementado. La implementación que se pide en el enunciado está indicada en los bloques <i>#TODO</i> y/o con variables igualadas a <i>None</i>."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:19:45.750797Z",
     "start_time": "2024-12-07T15:19:45.737988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "start = \"2019-01-01\"\n",
    "end = \"2021-01-01\"\n",
    "ticker = \"SPY\"\n",
    "initial_balance = 10_000\n",
    "\n",
    "class StockMarketEnv(gym.Env):\n",
    "    def __init__(\n",
    "            self, \n",
    "            ticker=ticker, \n",
    "            initial_balance=initial_balance,  \n",
    "            is_eval = False,\n",
    "            start = start,\n",
    "            end = end,\n",
    "            save_to_csv=False,\n",
    "            csv_filename=\"stock_trading_log.csv\"\n",
    "    ):\n",
    "        super(StockMarketEnv, self).__init__()\n",
    "\n",
    "        # Descargar los datos históricos de la acción\n",
    "        self.df = yf.download(ticker, start, end)\n",
    "        self.num_trading_days = len(self.df)\n",
    "        self.prices = self.df.Close.values\n",
    "        self.n_steps = len(self.prices)-1\n",
    "\n",
    "        # Parámetros del entorno\n",
    "        self.initial_balance = initial_balance\n",
    "        self.current_step = 0\n",
    "        self.balance = initial_balance\n",
    "        self.shares_held = 0\n",
    "        self.net_worth = initial_balance\n",
    "        self.previus_net_worth = initial_balance\n",
    "\n",
    "        # Espacio de acciones: 0 -> mantener, 1 -> comprar, 2 -> vender\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "\n",
    "        # Calculamos los indicadores técnicos\n",
    "        self.rsi = calculate_rsi(self.df.Close).values\n",
    "        self.ema = calculate_ema(self.df.Close).values\n",
    "\n",
    "\n",
    "        # Espacio de observaciones: [precio_actual, balance, acciones, rsi, ema, sma, upper_band, lower_band]\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(5,),  # 5 features: price, balance, shares, rsi, ema\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.is_eval = is_eval\n",
    "\n",
    "        # Valores para normalización (obtenemos mínimos y máximos)\n",
    "        self.min_price = self.prices.min()\n",
    "        self.max_price = self.prices.max()\n",
    "        self.min_rsi = self.rsi.min()\n",
    "        self.max_rsi = self.rsi.max()\n",
    "        self.min_ema = self.ema.min()\n",
    "        self.max_ema = self.ema.max()\n",
    "\n",
    "\n",
    "        # Parámetros adicionales para el CSV\n",
    "        self.save_to_csv = save_to_csv\n",
    "        self.csv_filename = csv_filename\n",
    "\n",
    "        # Si la opción de almacenar en CSV está activada, crea o sobreescribe el archivo\n",
    "        if self.save_to_csv:\n",
    "            pass\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.current_step = 0\n",
    "        self.balance = self.initial_balance\n",
    "        self.shares_held = 0\n",
    "        self.net_worth = self.initial_balance\n",
    "        self.previus_net_worth = self.initial_balance\n",
    "        \n",
    "        if seed is not None:\n",
    "            super().reset(seed=seed)\n",
    "        \n",
    "        return self._next_observation(), {}\n",
    "\n",
    "    def _normalize(self, value, min_val, max_val):\n",
    "        return (value - min_val) / (max_val - min_val)\n",
    "\n",
    "    def _next_observation(self):\n",
    "        # Normalizamos los valores\n",
    "        current_price = self.prices[self.current_step][0]\n",
    "        norm_price = self._normalize(current_price, self.min_price, self.max_price)\n",
    "        norm_balance = self._normalize(self.balance, self.initial_balance * 0.85, self.initial_balance * 1.25)\n",
    "        norm_shares_held = self._normalize(self.shares_held, 0, 100)  # Máximo de 100 acciones\n",
    "        norm_rsi = self._normalize(self.rsi[self.current_step], self.min_rsi, self.max_rsi)\n",
    "        norm_ema = self._normalize(self.ema[self.current_step], self.min_ema, self.max_ema)\n",
    "\n",
    "        return np.array([\n",
    "            norm_price,\n",
    "            norm_balance,\n",
    "            norm_shares_held,\n",
    "            norm_rsi,\n",
    "            norm_ema,\n",
    "        ])\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.prices[self.current_step][0]\n",
    "        self.previus_net_worth = self.net_worth\n",
    "\n",
    "        if action == 1:  # Buy\n",
    "            max_shares_possible = self.balance // current_price\n",
    "            shares_to_buy = max_shares_possible\n",
    "            \n",
    "            purchase_cost = shares_to_buy * current_price\n",
    "            self.balance -= purchase_cost\n",
    "            self.shares_held += shares_to_buy\n",
    "            \n",
    "        elif action == 2:  # Sell\n",
    "            sale_value = self.shares_held * current_price\n",
    "            self.balance += sale_value\n",
    "            self.shares_held = 0\n",
    "\n",
    "        self.net_worth = self.balance + (self.shares_held * current_price)\n",
    "        \n",
    "        step_reward = self.net_worth - self.previus_net_worth\n",
    "        \n",
    "        self.current_step += 1\n",
    "        \n",
    "        is_terminated = self.current_step >= self.n_steps\n",
    "        is_truncated = self.net_worth < (self.initial_balance * 0.85) \n",
    "        \n",
    "        if self.save_to_csv:\n",
    "            self.save_to_csv_file()\n",
    "        \n",
    "        obs = self._next_observation()\n",
    "        \n",
    "        info = {\n",
    "            'current_price': current_price,\n",
    "            'net_worth': self.net_worth,\n",
    "            'balance': self.balance,\n",
    "            'shares_held': self.shares_held,\n",
    "        }\n",
    "        \n",
    "        return obs, step_reward, is_terminated, is_truncated, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        profit = self.net_worth - self.initial_balance\n",
    "        \n",
    "        print(f'Step: {self.current_step}')\n",
    "        print(f'Balance: {self.balance:.2f}')\n",
    "        print(f'Shares held: {self.shares_held}')\n",
    "        print(f'Net worth: {self.net_worth:.2f}')\n",
    "        print(f'Profit: {profit:.2f}')\n",
    "        print('-' * 30)\n",
    "\n",
    "    # La función save_to_csv_file guarda los datos actuales en un archivo CSV.\n",
    "    # 1. Primero calcula el beneficio como la diferencia entre el valor neto\n",
    "    # actual y el balance inicial.\n",
    "    # 2. Luego, abre (o crea) el archivo CSV en modo 'append' para agregar una\n",
    "    # nueva fila de datos sin sobrescribir las anteriores.\n",
    "    # 3. Escribe una nueva fila en el CSV con los valores del paso actual,\n",
    "    # balance, acciones mantenidas, valor neto y el beneficio.\n",
    "    # Step,Balance,Shares Held,Net Worth,Profit\n",
    "    # 1,12000,50,13000,3000\n",
    "    def save_to_csv_file(self):\n",
    "        pass\n",
    "        \"\"\"Guarda los datos actuales en el archivo CSV.\"\"\"\n",
    "        profit = self.net_worth - self.initial_balance\n",
    "        data_to_write = {\n",
    "            'Step': self.current_step,\n",
    "            'Balance': round(self.balance, 2),\n",
    "            'Shares Held': self.shares_held,\n",
    "            'Net Worth': round(self.net_worth, 2),\n",
    "            'Profit': round(profit, 2)\n",
    "        }\n",
    "        \n",
    "        file_exists = os.path.isfile(self.csv_filename)\n",
    "        \n",
    "        with open(self.csv_filename, 'a', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=data_to_write.keys())\n",
    "            \n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "            \n",
    "            writer.writerow(data_to_write)"
   ],
   "outputs": [],
   "execution_count": 604
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlH3wQoIPqYv"
   },
   "source": [
    "La siguiente celda es de **comprobación** y debe generar la salida que se muestra a continuación. Esta salida sirve para verificar que todo se ha implementado correctamente. Al ejecutar la celda, la salida debe coincidir exactamente con el siguiente resultado:\n",
    "\n",
    "\n",
    "```\n",
    "------------------------------------------------------------------------------------\n",
    "(array([0.14086006, 0.375     , 0.        , 0.45140651, 0.        ]), 0, False, False, {})\n",
    "------------------------------------------------------------------------------------\n",
    "(array([ 0.19505732, -2.06710007,  0.4       ,  0.45140651,  0.00333123]), 0, False, False, {})\n",
    "------------------------------------------------------------------------------------\n",
    "(array([ 0.20824227, -2.06710007,  0.4       ,  0.45140651,  0.00842359]), 1, False, False, {})\n",
    "------------------------------------------------------------------------------------\n",
    "(array([0.22407732, 0.47669998, 0.        , 0.45140651, 0.01548553]), 1, False, False, {})\n",
    "------------------------------------------------------------------------------------\n",
    "Step: 4  \n",
    "Balance: 10406.799926757812  \n",
    "Shares held: 0  \n",
    "Net worth: 10406.799926757812  \n",
    "Profit: 406.7999267578125  \n",
    "None  \n",
    "------------------------------------------------------------------------------------\n",
    "(array([0.23202811, 0.47669998, 0.        , 0.45140651, 0.02293572]), -1, False, False, {})\n",
    "------------------------------------------------------------------------------------\n",
    "Step: 5  \n",
    "Balance: 10406.799926757812  \n",
    "Shares held: 0  \n",
    "Net worth: 10406.799926757812  \n",
    "Profit: 406.7999267578125  \n",
    "None  \n",
    "------------------------------------------------------------------------------------\n",
    "(array([ 0.23805742, -2.10300003,  0.4       ,  0.45140651,  0.03040101]), 0, False, False, {})\n",
    "------------------------------------------------------------------------------------\n",
    "Step: 6  \n",
    "Balance: 87.9998779296875  \n",
    "Shares held: 40  \n",
    "Net worth: 10406.799926757812  \n",
    "Profit: 406.7999267578125  \n",
    "None  \n",
    "------------------------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_c2GrpVHNMg3",
    "ExecuteTime": {
     "end_time": "2024-12-07T15:19:45.802761Z",
     "start_time": "2024-12-07T15:19:45.763120Z"
    }
   },
   "source": [
    "env = StockMarketEnv()\n",
    "env.reset()\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.step(0))\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.step(1))\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.step(1))\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.step(2))\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.render())\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.step(0))\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.render())\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.step(1))\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.render())\n",
    "print('------------------------------------------------------------------------------------')\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------\n",
      "(array([0.14086006, 0.375     , 0.        , 0.45140651, 0.        ]), np.float64(0.0), False, np.False_, {'current_price': np.float64(250.17999267578125), 'net_worth': np.float64(10000.0), 'balance': 10000, 'shares_held': 0})\n",
      "------------------------------------------------------------------------------------\n",
      "(array([ 0.19505732, -2.06710007,  0.4       ,  0.45140651,  0.00333123]), np.float64(0.0), False, np.False_, {'current_price': np.float64(244.2100067138672), 'net_worth': np.float64(10000.0), 'balance': np.float64(231.5997314453125), 'shares_held': np.float64(40.0)})\n",
      "------------------------------------------------------------------------------------\n",
      "(array([ 0.20824227, -2.06710007,  0.4       ,  0.45140651,  0.00842359]), np.float64(327.19970703125), False, np.False_, {'current_price': np.float64(252.38999938964844), 'net_worth': np.float64(10327.19970703125), 'balance': np.float64(231.5997314453125), 'shares_held': np.float64(40.0)})\n",
      "------------------------------------------------------------------------------------\n",
      "(array([0.22407732, 0.47669998, 0.        , 0.45140651, 0.01548553]), np.float64(79.6002197265625), False, np.False_, {'current_price': np.float64(254.3800048828125), 'net_worth': np.float64(10406.799926757812), 'balance': np.float64(10406.799926757812), 'shares_held': 0})\n",
      "------------------------------------------------------------------------------------\n",
      "Step: 4\n",
      "Balance: 10406.80\n",
      "Shares held: 0\n",
      "Net worth: 10406.80\n",
      "Profit: 406.80\n",
      "------------------------------\n",
      "None\n",
      "------------------------------------------------------------------------------------\n",
      "(array([0.23202811, 0.47669998, 0.        , 0.45140651, 0.02293572]), np.float64(0.0), False, np.False_, {'current_price': np.float64(256.7699890136719), 'net_worth': np.float64(10406.799926757812), 'balance': np.float64(10406.799926757812), 'shares_held': 0})\n",
      "------------------------------------------------------------------------------------\n",
      "Step: 5\n",
      "Balance: 10406.80\n",
      "Shares held: 0\n",
      "Net worth: 10406.80\n",
      "Profit: 406.80\n",
      "------------------------------\n",
      "None\n",
      "------------------------------------------------------------------------------------\n",
      "(array([ 0.23805742, -2.10300003,  0.4       ,  0.45140651,  0.03040101]), np.float64(0.0), False, np.False_, {'current_price': np.float64(257.9700012207031), 'net_worth': np.float64(10406.799926757812), 'balance': np.float64(87.9998779296875), 'shares_held': np.float64(40.0)})\n",
      "------------------------------------------------------------------------------------\n",
      "Step: 6\n",
      "Balance: 88.00\n",
      "Shares held: 40.0\n",
      "Net worth: 10406.80\n",
      "Profit: 406.80\n",
      "------------------------------\n",
      "None\n",
      "------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 605
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmlDKL_WzJXi"
   },
   "source": [
    " #### 1.1.3 Interacción con el Entorno StockMarketEnv\n",
    "\n",
    " <div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Ejercicio 3 (0.5 ptos):</strong> Cargar el entorno <code>StockMarketEnv</code> y realizar las siguientes tareas:\n",
    "     <ul> <li>Mostrar el espacio de acciones y el espacio de observaciones.</li>\n",
    "     <li>Ejecutar 100 episodios con acciones aleatorias, mostrando la recompensa media obtenida entre todas las partidas.</li>\n",
    "      <li> Ejecuta una partida aleatoria y mostrar el render al finalizar la misma.</li>\n",
    "      <li>Probar la función <code>save_to_csv_file</code> y mostrar el resultado de las últimas 5 filas.</li>\n",
    "     </ul>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Pt0BhGGJzJXi",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-12-07T15:19:45.881685Z",
     "start_time": "2024-12-07T15:19:45.848261Z"
    }
   },
   "source": "env = StockMarketEnv()",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "execution_count": 606
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tldZjjBlzJXi",
    "ExecuteTime": {
     "end_time": "2024-12-07T15:19:45.919457Z",
     "start_time": "2024-12-07T15:19:45.917349Z"
    }
   },
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(3)\n",
      "Box(-inf, inf, (5,), float32)\n"
     ]
    }
   ],
   "execution_count": 607
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UxM9K02yNMg3",
    "ExecuteTime": {
     "end_time": "2024-12-07T15:19:46.223453Z",
     "start_time": "2024-12-07T15:19:45.942495Z"
    }
   },
   "source": [
    "episodes = 100\n",
    "\n",
    "env = StockMarketEnv()\n",
    "total_rewards = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Random action for demonstration; replace with your policy\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        done = terminated or truncated\n",
    "\n",
    "    total_rewards.append(episode_reward)\n",
    "    print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "\n",
    "mean_reward = np.mean(total_rewards)\n",
    "print(f\"Mean Reward over {episodes} episodes: {mean_reward}\")\n",
    "\n",
    "print(f'\\nMean reward over {episodes} episodes: {mean_reward}')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = -70.80477905273438\n",
      "Episode 2: Reward = 4053.454559326172\n",
      "Episode 3: Reward = 7450.728042602539\n",
      "Episode 4: Reward = 3201.335662841797\n",
      "Episode 5: Reward = 3331.7330932617188\n",
      "Episode 6: Reward = 1155.3042602539062\n",
      "Episode 7: Reward = 3630.909912109375\n",
      "Episode 8: Reward = 4901.882659912109\n",
      "Episode 9: Reward = 1430.3616027832031\n",
      "Episode 10: Reward = 1718.9289093017578\n",
      "Episode 11: Reward = -2084.3814697265625\n",
      "Episode 12: Reward = -1529.9939422607422\n",
      "Episode 13: Reward = -1864.2590026855469\n",
      "Episode 14: Reward = 2436.7086486816406\n",
      "Episode 15: Reward = 1741.261001586914\n",
      "Episode 16: Reward = 3897.890625\n",
      "Episode 17: Reward = 5835.312255859375\n",
      "Episode 18: Reward = -1818.4203033447266\n",
      "Episode 19: Reward = 3457.9021911621094\n",
      "Episode 20: Reward = 5095.670196533203\n",
      "Episode 21: Reward = 1972.3536224365234\n",
      "Episode 22: Reward = 1835.7974395751953\n",
      "Episode 23: Reward = 5310.430755615234\n",
      "Episode 24: Reward = 5698.232086181641\n",
      "Episode 25: Reward = 322.4954376220703\n",
      "Episode 26: Reward = 4967.844055175781\n",
      "Episode 27: Reward = 3562.790786743164\n",
      "Episode 28: Reward = 1657.791519165039\n",
      "Episode 29: Reward = -1808.998031616211\n",
      "Episode 30: Reward = 491.8576965332031\n",
      "Episode 31: Reward = 2259.768264770508\n",
      "Episode 32: Reward = -1682.4498291015625\n",
      "Episode 33: Reward = -1803.950927734375\n",
      "Episode 34: Reward = 6895.9970703125\n",
      "Episode 35: Reward = 2017.5635223388672\n",
      "Episode 36: Reward = 3949.363006591797\n",
      "Episode 37: Reward = 2416.9200897216797\n",
      "Episode 38: Reward = 2290.667755126953\n",
      "Episode 39: Reward = 2149.027297973633\n",
      "Episode 40: Reward = -1784.6909790039062\n",
      "Episode 41: Reward = 4413.66667175293\n",
      "Episode 42: Reward = 4176.657913208008\n",
      "Episode 43: Reward = -1557.9700927734375\n",
      "Episode 44: Reward = -1763.7366638183594\n",
      "Episode 45: Reward = -2025.0990295410156\n",
      "Episode 46: Reward = -1521.2953491210938\n",
      "Episode 47: Reward = 2160.5267333984375\n",
      "Episode 48: Reward = -42.844268798828125\n",
      "Episode 49: Reward = 483.1662292480469\n",
      "Episode 50: Reward = 2677.3756713867188\n",
      "Episode 51: Reward = -1771.9332580566406\n",
      "Episode 52: Reward = 8534.119735717773\n",
      "Episode 53: Reward = 870.6271057128906\n",
      "Episode 54: Reward = 5006.037902832031\n",
      "Episode 55: Reward = -1798.2488555908203\n",
      "Episode 56: Reward = 2986.697952270508\n",
      "Episode 57: Reward = 1547.6930847167969\n",
      "Episode 58: Reward = 4614.882583618164\n",
      "Episode 59: Reward = -1621.3172912597656\n",
      "Episode 60: Reward = 4462.358779907227\n",
      "Episode 61: Reward = -1910.6380004882812\n",
      "Episode 62: Reward = 4515.112106323242\n",
      "Episode 63: Reward = -115.99786376953125\n",
      "Episode 64: Reward = 2735.8787231445312\n",
      "Episode 65: Reward = 6224.09065246582\n",
      "Episode 66: Reward = 2254.8006286621094\n",
      "Episode 67: Reward = -1803.1070556640625\n",
      "Episode 68: Reward = 393.5918426513672\n",
      "Episode 69: Reward = -1672.7653350830078\n",
      "Episode 70: Reward = 3089.7186737060547\n",
      "Episode 71: Reward = 2596.2299194335938\n",
      "Episode 72: Reward = -1871.4857177734375\n",
      "Episode 73: Reward = 3002.987060546875\n",
      "Episode 74: Reward = 1633.0640716552734\n",
      "Episode 75: Reward = 3029.6090698242188\n",
      "Episode 76: Reward = -1562.1838836669922\n",
      "Episode 77: Reward = 2360.4297790527344\n",
      "Episode 78: Reward = 836.5308685302734\n",
      "Episode 79: Reward = 3466.080764770508\n",
      "Episode 80: Reward = -1673.3584594726562\n",
      "Episode 81: Reward = 3763.6831665039062\n",
      "Episode 82: Reward = 3688.245086669922\n",
      "Episode 83: Reward = 3409.427703857422\n",
      "Episode 84: Reward = 2147.7364349365234\n",
      "Episode 85: Reward = 4511.069671630859\n",
      "Episode 86: Reward = -180.00096130371094\n",
      "Episode 87: Reward = 4396.347198486328\n",
      "Episode 88: Reward = -1683.3135681152344\n",
      "Episode 89: Reward = 4407.0621337890625\n",
      "Episode 90: Reward = 3468.2914276123047\n",
      "Episode 91: Reward = 4185.099014282227\n",
      "Episode 92: Reward = 4018.8719940185547\n",
      "Episode 93: Reward = 1234.1734313964844\n",
      "Episode 94: Reward = -1827.9817657470703\n",
      "Episode 95: Reward = 1990.7967224121094\n",
      "Episode 96: Reward = 3595.6587982177734\n",
      "Episode 97: Reward = 1796.7228393554688\n",
      "Episode 98: Reward = 7293.210037231445\n",
      "Episode 99: Reward = 2488.159637451172\n",
      "Episode 100: Reward = 4423.9010009765625\n",
      "Mean Reward over 100 episodes: 1991.7744816589357\n",
      "\n",
      "Mean reward over 100 episodes: 1991.7744816589357\n"
     ]
    }
   ],
   "execution_count": 608
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EK54MX40NMg3",
    "ExecuteTime": {
     "end_time": "2024-12-07T15:19:46.238347Z",
     "start_time": "2024-12-07T15:19:46.236171Z"
    }
   },
   "source": [
    "import time\n",
    "env.render()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 504\n",
      "Balance: 288.28\n",
      "Shares held: 38.0\n",
      "Net worth: 14423.90\n",
      "Profit: 4423.90\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 609
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HqRtVBGI1RVs",
    "ExecuteTime": {
     "end_time": "2024-12-07T15:19:46.332308Z",
     "start_time": "2024-12-07T15:19:46.262087Z"
    }
   },
   "source": [
    "def probar_save_to_csv(env, rows ):\n",
    "    \"\"\"\n",
    "    Probar la función save_to_csv_file ejecutando varias acciones\n",
    "    en el entorno y luego mostrando las últimas 5 filas del archivo CSV.\n",
    "\n",
    "    Parámetros:\n",
    "        env: El entorno StockMarketEnv.\n",
    "        num_steps: El número de pasos que se desean ejecutar.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Random action\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    if env.save_to_csv:\n",
    "        try:\n",
    "            df = pd.read_csv(env.csv_filename)\n",
    "            print(f\"\\nLast {rows} rows of {env.csv_filename}:\")\n",
    "            print(df.tail(rows))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File {env.csv_filename} not found.\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Error: File {env.csv_filename} is empty.\")\n",
    "\n",
    "env = StockMarketEnv(ticker=\"SPY\", start=\"2019-01-01\", end=\"2021-01-01\", save_to_csv=True, csv_filename=\"stock_trading_log.csv\")\n",
    "probar_save_to_csv(env, 5)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Last 5 rows of stock_trading_log.csv:\n",
      "      Step     Balance  Shares Held   Net Worth     Profit\n",
      "1003   500   212.91000     32.00000 11975.15000 1975.15000\n",
      "1004   501   212.91000     32.00000 12020.91000 2020.91000\n",
      "1005   502   212.91000     32.00000 12122.35000 2122.35000\n",
      "1006   503   212.91000     32.00000 12099.63000 2099.63000\n",
      "1007   504 12116.59000      0.00000 12116.59000 2116.59000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 610
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_ZXX9atzJXj"
   },
   "source": [
    "## 2. Agente DQN inversor en bolsa (2 ptos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CgCRw341JS7"
   },
   "source": [
    "En este apartado implementaremos una DQN teniendo en cuenta la exploración-explotación (epsilon-*greedy*), la red objetivo, y el buffer de repetición de experiencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90l7Ba721JS7"
   },
   "source": [
    "Definiremos el buffer como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hPdwD1G81JS7",
    "ExecuteTime": {
     "end_time": "2024-12-07T15:19:46.367257Z",
     "start_time": "2024-12-07T15:19:46.363502Z"
    }
   },
   "source": [
    "class experienceReplayBuffer:\n",
    "\n",
    "    def __init__(self, memory_size=50000, burn_in=10000):\n",
    "        self.memory_size = memory_size\n",
    "        self.burn_in = burn_in\n",
    "        self.buffer = namedtuple('Buffer',\n",
    "            field_names=['state', 'action', 'reward', 'done', 'next_state'])\n",
    "        self.replay_memory = deque(maxlen=memory_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        samples = np.random.choice(len(self.replay_memory), batch_size,\n",
    "                                   replace=False)\n",
    "        # Use asterisk operator to unpack deque\n",
    "        batch = zip(*[self.replay_memory[i] for i in samples])\n",
    "        return batch\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.replay_memory.append(\n",
    "            self.buffer(state, action, reward, done, next_state))\n",
    "\n",
    "    def burn_in_capacity(self):\n",
    "        return len(self.replay_memory) / self.burn_in"
   ],
   "outputs": [],
   "execution_count": 611
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvY3iwTP1JS7"
   },
   "source": [
    "### 2.1 Implementación de la Clase NeuralNetStockMarket\n",
    "\n",
    "<br>Primeramente implementaremos la red neuronal, utilizando un modelo Secuencial con la siguiente configuración:</br>\n",
    "<ul>\n",
    "<li>\n",
    "Tres capas completamente conectadas (representadas en pytorch por nn.Lineal) con 256, 128 y 64 neuronas cada una, bias=True, y activación ReLU</li>\n",
    "<li>Una capa de salida completamente conectada y bias=True</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISy3TJ5s1JS7"
   },
   "source": [
    "Usaremos el optimizador Adam para entrenar la red.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-FNzJNO1JS8"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Ejercicio (0.5 ptos):</strong> Implementar la clase <code>NeuralNetStockMarket:()</code>. Inicializar las variables necesarias y definir el modelo Secuencial de red neuronal indicado.\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se os proporciona el código pre-implementado. La implementación que se pide en el enunciado está indicada en los bloques <i>#TODO</i> y/o con variables igualadas a <i>None</i>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ql0g4IKE1JS8",
    "ExecuteTime": {
     "end_time": "2024-12-07T15:19:46.393009Z",
     "start_time": "2024-12-07T15:19:46.388021Z"
    }
   },
   "source": [
    "\n",
    "class NeuralNetStockMarket(torch.nn.Module):\n",
    "\n",
    "    ###################################\n",
    "    ###inicialización y modelo###\n",
    "    def __init__(self, env, learning_rate=1e-3, optimizer = None, device=None):\n",
    "        super(NeuralNetStockMarket, self).__init__()\n",
    "        self.device = torch.device(\"mps\")\n",
    "        self.n_inputs = env.observation_space.shape[0]\n",
    "        self.n_outputs = env.action_space.n\n",
    "        self.actions = np.arange(env.action_space.n)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.n_inputs, 256, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            torch.nn.Linear(256, 128, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            torch.nn.Linear(128, 64, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            torch.nn.Linear(64, self.n_outputs, bias=True)\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Initialize optimizer\n",
    "        if optimizer is None:\n",
    "            self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "\n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(self.actions)  # acció aleatòria\n",
    "        else:\n",
    "            qvals = self.get_qvals(state)  # acció a partir del càlcul del valor de Q per a aquesta acció\n",
    "            action = torch.max(qvals, dim=-1)[1].item()\n",
    "        return action\n",
    "    \n",
    "    def get_qvals(self, state):\n",
    "        if type(state) is tuple:\n",
    "            state = np.array([np.ravel(s) for s in state])\n",
    "        state_t = torch.FloatTensor(state).to(self.device)\n",
    "        return self.model(state_t)"
   ],
   "outputs": [],
   "execution_count": 612
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEfgOTll1JS8"
   },
   "source": [
    "### 2.2 Implementación del Agente DQN con Exploración/Explotación y Sincronización de Redes\n",
    "\n",
    "A continuación implementaremos una clase que defina el comportamiento del agente DQN teniendo en cuenta:\n",
    "\n",
    "La exploración/explotación (decaimiento de epsilon)\n",
    "La actualización y sincronización de la red principal y la red objetivo (pérdida)\n",
    "Consideraremos que el agente ha aprendido a realizar la tarea (i.e. el \"juego\" termina) cuando obtiene una media de mínimo 8700$ durante 100 episodios consecutivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQVGVM6T1JS8"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.5 ptos):</strong> Implementar los siguientes puntos de la clase <code>DQNAgent()</code>:\n",
    "    <ol>\n",
    "        <li>Declarar las variables de la clase</li>\n",
    "        <li>Inicializar las variables necesarias</li>\n",
    "        <li>Implementar la acción a tomar</li>\n",
    "        <li>Actualizar la red principal según la frecuencia establecida en los hiperparámetros</li>\n",
    "        <li>Calcular la pérdida (ecuación Bellman, etc)</li>\n",
    "        <li>Sincronizar la red objetivo según la frecuencia establecida en los hiperparámetros</li>\n",
    "        <li>Calcular la media de recompensas de los últimos 100 episodios</li>\n",
    "         <li>Comprobar límite de episodios</li>\n",
    "        <li>Actualizar epsilon según: $$ \\textrm{max}(\\epsilon · \\epsilon_{\\textrm{decay}}, 0.01) $$ </li>\n",
    "    </ol>\n",
    "Además, durante el proceso se deben almacenar (*):\n",
    "    <ol>\n",
    "        <li>Las recompensas obtenidas en cada paso del entrenamiento</li>\n",
    "        <li>Las recompensas medias cada 100 episodios</li>\n",
    "        <li>La pérdida durante el entrenamiento</li>\n",
    "        <li>La evolución de epsilon a lo largo del entrenamiento</li>\n",
    "                 <li>Almacena la cantidad de episodios necesarios para llevar acabo el entrenamiento en la variable episodes_train_dqn </li>\n",
    "    </ol>\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se os proporciona el código pre-implementado. La implementación que se pide en el enunciado está indicada en los bloques <i>#TODO</i> y/o con variables igualadas a <i>None</i>, salvo (*) en qué momento almacenar las variables que se indican.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HQcTPvAE1JS8",
    "ExecuteTime": {
     "end_time": "2024-12-07T15:19:46.419985Z",
     "start_time": "2024-12-07T15:19:46.407142Z"
    }
   },
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env, main_network, buffer, epsilon=0.1, eps_decay=0.99, batch_size=32, min_episodes= 300, device = None):\n",
    "        ######################################\n",
    "        ##TODO 1: Declarar variables\n",
    "        self.env = env\n",
    "        self.main_network = main_network\n",
    "        self.target_network = deepcopy(main_network) # red objetivo (copia de la principal)\n",
    "        self.buffer = buffer\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.nblock = 100 # bloque de los X últimos episodios de los que se calculará la media de recompensa\n",
    "        self.initialize()\n",
    "        self.episodes_train_dqn = 0\n",
    "        self.min_episodes = min_episodes\n",
    "        self.device = torch.device(\"mps\")\n",
    "        # self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Configurar el dispositivo (CPU o GPU)\n",
    "\n",
    "\n",
    "    def initialize(self):\n",
    "        ######################################\n",
    "        ##TODO 3: Inicializa lo necesario\n",
    "        self.update_loss = []\n",
    "        self.training_rewards = []\n",
    "        self.mean_training_rewards = []\n",
    "        self.epsilon_history = []  # Store epsilon values\n",
    "        self.sync_eps = []\n",
    "        self.total_reward = 0\n",
    "        self.step_count = 0\n",
    "        self.state0 = self.env.reset()[0]\n",
    "\n",
    "    ## Tomar nueva acción\n",
    "    def take_step(self, eps, mode='train'):\n",
    "        if mode == 'explore':\n",
    "            # acción aleatoria en el burn-in y en la fase de exploración (epsilon)>\n",
    "            action = self.env.action_space.sample() \n",
    "        else:\n",
    "            # acción a partir del valor de Q (elección de la acción con mejor Q)\n",
    "            action = self.main_network.get_action(self.state0, eps)\n",
    "            self.step_count += 1\n",
    "        #TODO: tomar 'step' i obtener nuevo estado y recompensa. Guardar la experiencia en el buffer\n",
    "        new_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        # new_state, reward, done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "        self.buffer.append(self.state0, action, reward, done, new_state) # guardem experiència en el buffer\n",
    "        self.state0 = new_state.copy()\n",
    "        \n",
    "        #TODO: resetear entorno 'if done'\n",
    "        if done:\n",
    "            self.training_rewards.append(self.total_reward)\n",
    "            self.total_reward = 0\n",
    "            self.state0 = env.reset()[0]\n",
    "        return done\n",
    "\n",
    "\n",
    "\n",
    "    ## Entrenamiento\n",
    "    def train(self, gamma=0.99, max_episodes=50000,\n",
    "              batch_size=32,\n",
    "              dnn_update_frequency=4,\n",
    "              dnn_sync_frequency=2000, REWARD_THRESHOLD = 9000):\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Rellenamos el buffer con N experiencias aleatorias ()\n",
    "        print(\"Filling replay buffer...\")\n",
    "        while self.buffer.burn_in_capacity() < 1:\n",
    "            self.take_step(self.epsilon, mode='explore')\n",
    "\n",
    "\n",
    "        episode = 0\n",
    "        training = True\n",
    "        print(\"Training...\")\n",
    "        while training:\n",
    "            self.state0 = self.env.reset()[0]\n",
    "            self.total_reward = 0\n",
    "            gamedone = False\n",
    "            while gamedone == False:\n",
    "                # El agente toma una acción\n",
    "                gamedone = self.take_step(self.epsilon, mode='train')\n",
    "                ##################################################################################\n",
    "                #####TODO 4:  Actualizar la red principal según la frecuencia establecida #######\n",
    "                if self.step_count % dnn_update_frequency == 0:\n",
    "                    self.update()\n",
    "                ########################################################################################\n",
    "                ###TODO 6: Sincronizar red principal y red objetivo según la frecuencia establecida#####\n",
    "                if self.step_count % dnn_sync_frequency == 0:\n",
    "                    self.target_network.load_state_dict(\n",
    "                        self.main_network.state_dict())\n",
    "                    self.sync_eps.append(episode)\n",
    "\n",
    "\n",
    "                if gamedone:\n",
    "                    episode += 1\n",
    "                    #######################################################################################\n",
    "                    ###TODO 7: calcular la media de recompensa de los últimos X episodios, y almacenar#####\n",
    "                    self.episodes_train_dqn = episode \n",
    "                    self.update_loss = []\n",
    "                    mean_rewards = np.mean(   # calculem la mitjana de recompensa dels últims X episodis\n",
    "                        self.training_rewards[-self.nblock:])\n",
    "                    self.mean_training_rewards.append(mean_rewards)\n",
    "                    \n",
    "                    # Store epsilon\n",
    "                    self.epsilon_history.append(self.epsilon)\n",
    "\n",
    "                    #######################################################################################\n",
    "                    ### TODO 8: Comprobar que todavía quedan episodios. Parar el aprendizaje si se llega al límite\n",
    "\n",
    "                    print(\"\\rEpisode {:d} Mean Rewards {:.2f} Epsilon {}\\t\\t\".format(\n",
    "                        episode, mean_rewards, self.epsilon), end=\"\")\n",
    "\n",
    "                    # Comprobamos que todavía quedan episodios\n",
    "                    if episode >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                            episode))\n",
    "                        break\n",
    "\n",
    "                    #######################################################################################\n",
    "                    ### TODO 9: Añadir min episodes\n",
    "                    if mean_rewards >= REWARD_THRESHOLD and self.min_episodes < episode :\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                            episode))\n",
    "                        break\n",
    "\n",
    "\n",
    "                    #################################################################################\n",
    "                    ######TODO 9: Actualizar epsilon según la velocidad de decaimiento fijada########\n",
    "                    self.epsilon = max(self.epsilon * self.eps_decay, 0.01)\n",
    "\n",
    "    ## Cálculo de la pérdida\n",
    "    ## Càlcul de la pèrdua                   \n",
    "    def calculate_loss(self, batch):\n",
    "      #  print('loss')\n",
    "        # Separamos las variables de la experiencia y las convertimos a tensores\n",
    "        states, actions, rewards, dones, next_states = [i for i in batch]\n",
    "        rewards_vals = torch.FloatTensor(rewards).to(self.device)\n",
    "        actions_vals = torch.LongTensor(np.array(actions)).reshape(-1,1).to(self.device)\n",
    "        dones_t = torch.tensor(dones, dtype=torch.bool).to(self.device)\n",
    "\n",
    "        # Obtenemos los valores de Q de la red principal\n",
    "        qvals = torch.gather(self.main_network.get_qvals(states), 1, actions_vals).to(self.device)\n",
    "        # Obtenemos los valores de Q objetivo. El parámetro detach() evita que estos valores actualicen la red objetivo\n",
    "        qvals_next = torch.max(self.target_network.get_qvals(next_states),\n",
    "                               dim=-1)[0].detach().to(self.device)\n",
    "        qvals_next[dones_t.bool()] = 0\n",
    "\n",
    "        #################################################################################\n",
    "        ### TODO: Calcular ecuación de Bellman\n",
    "        expected_q_values = rewards_vals + (self.gamma * qvals_next)  # Shape: (batch_size,)\n",
    "        # expected_qvals = None\n",
    "\n",
    "        #################################################################################\n",
    "        ### TODO: Calcular la pérdida (MSE)\n",
    "        loss = torch.nn.MSELoss()(qvals, expected_q_values.reshape(-1,1))\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        self.main_network.optimizer.zero_grad()  # eliminamos cualquier gradiente pasado\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size) # seleccionamos un conjunto del buffer\n",
    "        loss = self.calculate_loss(batch) # calculamos la pérdida\n",
    "        loss.backward() # hacemos la diferencia para obtener los gradientes\n",
    "        self.main_network.optimizer.step() # aplicamos los gradientes a la red neuronal\n",
    "        # Guardamos los valores de pérdida\n",
    "        if self.device.type == 'mps':\n",
    "            loss_value = loss.detach().to('cpu').item()\n",
    "        else:\n",
    "            loss_value = loss.detach().cpu().numpy()\n",
    "        self.update_loss.append(loss_value)\n"
   ],
   "outputs": [],
   "execution_count": 613
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70dAg7v71JS9"
   },
   "source": [
    "### 2.3 Entrenamiento del Modelo\n",
    "\n",
    "\n",
    "(0.5 ptos) A continuación entrenaremos el modelo con los siguientes hiperparámetros:\n",
    "   <ul>\n",
    "        <li>Velocidad de aprendizaje: 0.0005</li>\n",
    "        <li>Tamaño del batch: 128</li>\n",
    "        <li>Número de episodios: 4000</li>\n",
    "        <li>Número de episodios para rellenar el buffer (BURN_IN): 1000</li>\n",
    "        <li>Frecuencia de actualización de la red neuronal: 6 </li>\n",
    "        <li>Frecuencia de sincronización con la red objetivo: 15</li>\n",
    "        <li>Capacidad máxima del buffer (MEMORY_SIZE ): 50000</li>\n",
    "        <li>Factor de descuento: 0.99</li>\n",
    "        <li>Epsilon: 1, con decaimiento de 0.995</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yBFZNwKA1JS9",
    "ExecuteTime": {
     "end_time": "2024-12-07T15:19:46.434310Z",
     "start_time": "2024-12-07T15:19:46.431596Z"
    }
   },
   "source": [
    "#TODO deficinición de variables.\n",
    "LEARNING_RATE = 0.0005\n",
    "BATCH_SIZE = 128\n",
    "MAX_EPISODES = 4000\n",
    "BURN_IN = 1000\n",
    "UPDATE_FREQ = 6\n",
    "SYNC_FREQ = 15\n",
    "MEMORY_SIZE = 50000\n",
    "GAMMA = 0.99\n",
    "EPSILON = 1.0\n",
    "EPSILON_DECAY = 0.995"
   ],
   "outputs": [],
   "execution_count": 614
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7kU6ANy1NMg8",
    "ExecuteTime": {
     "end_time": "2024-12-07T15:19:46.475996Z",
     "start_time": "2024-12-07T15:19:46.445715Z"
    }
   },
   "source": [
    "#TODO calcular el REWARD_THRESHOLD\n",
    "ticker = 'SPY'\n",
    "start = '2019-01-01'\n",
    "end = '2021-01-01'\n",
    "# Download data to get number of trading days\n",
    "temp_data = yf.download(ticker, start, end)\n",
    "num_days = len(temp_data)\n",
    "print(f\"Numero de dias de trading para {ticker} desde {start} hasta {end}: {num_days}\")\n",
    "print(f\"Nuestro objetivo ganar el 50 por ciento de los dias: {round(num_days/2)}\")\n",
    "REWARD_THRESHOLD = 8_700\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de dias de trading para SPY desde 2019-01-01 hasta 2021-01-01: 505\n",
      "Nuestro objetivo ganar el 50 por ciento de los dias: 252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 615
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kBl792X11JS9",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-07T15:19:46.523034Z"
    }
   },
   "source": [
    "import numpy\n",
    "#TODO entrenamiento..\n",
    "#Training time en Google Colab en GPU: 42.53 minutes\n",
    "#De media obtiene enre 170-190 de puntuación y alcanza los 4000 episodios.\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create environment\n",
    "env = StockMarketEnv(ticker=ticker, start=start, end=end)\n",
    "\n",
    "# Create experience replay buffer\n",
    "buffer = experienceReplayBuffer(memory_size=MEMORY_SIZE, burn_in=BURN_IN)\n",
    "\n",
    "# Create neural network\n",
    "main_network = NeuralNetStockMarket(\n",
    "    env=env,\n",
    "    learning_rate=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Create DQN agent\n",
    "agent = DQNAgent(\n",
    "    env=env,\n",
    "    main_network=main_network,\n",
    "    buffer=buffer,\n",
    "    epsilon=EPSILON,\n",
    "    eps_decay=EPSILON_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    min_episodes=1000\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "agent.train(\n",
    "    gamma=GAMMA,\n",
    "    max_episodes=MAX_EPISODES,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    dnn_update_frequency=UPDATE_FREQ,\n",
    "    dnn_sync_frequency=SYNC_FREQ,\n",
    "    REWARD_THRESHOLD=REWARD_THRESHOLD\n",
    ")\n",
    "\n",
    "# Calculate training time\n",
    "end_time = time.time()\n",
    "training_time = (end_time - start_time) / 60  # Convert to minutes\n",
    "print(f\"\\nTraining time: {training_time:.2f} minutes\")\n",
    "\n",
    "# Print final results\n",
    "print(f\"Episodes completed: {agent.episodes_train_dqn}\")\n",
    "print(f\"Final mean reward: {agent.mean_training_rewards[-1]:.2f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling replay buffer...\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11 Mean Rewards 2087.88 Epsilon 0.9511101304657719\t\t"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff6qg_fV1JS9"
   },
   "source": [
    "### 2.4 Análisis del entrenamiento\n",
    "\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.25 ptos):</strong> Representar:\n",
    "    <ol>\n",
    "        <li>Gráfico con las recompensas obtenidas a lo largo del entrenamieno, la evolución de las recompensas medias cada 100 episodios, y el umbral de recompensa establecido por el entorno.</li>\n",
    "        <li>Gráfico con la evolución de la perdida a lo largo del entrenamiento</li>\n",
    "        <li>Gráfico con la evolución de epsilon a lo largo del entrenamiento</li>\n",
    "    </ol>\n",
    "\n",
    "Comentar los resultados obtenidos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "A6tyd9Z11JS-",
    "ExecuteTime": {
     "end_time": "2024-12-07T15:19:38.296992Z",
     "start_time": "2024-12-07T15:10:15.542711Z"
    }
   },
   "source": [
    "def plot_rewards():\n",
    "    print(\"aaa\")\n",
    "    #TODO\n",
    "    pass\n",
    "\n",
    "plot_rewards()"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[578], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;66;03m#TODO\u001B[39;00m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[43mplot_rewards\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[578], line 2\u001B[0m, in \u001B[0;36mplot_rewards\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplot_rewards\u001B[39m():\n\u001B[0;32m----> 2\u001B[0m     \u001B[38;5;28;43mprint\u001B[39;49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maaa\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;66;03m#TODO\u001B[39;00m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_311_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_311_64.SafeCallWrapper.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_311_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_311_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_311_64.pyx:937\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_311_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_311_64.pyx:928\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_311_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_311_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_311_64.PyDBFrame.do_wait_suspend\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py:1220\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1217\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1219\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1220\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py:1235\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1232\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1234\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1235\u001B[0m         time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[1;32m   1237\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1239\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 578
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwBsmpUz1JS-"
   },
   "outputs": [],
   "source": [
    "def plot_loss(tr_loss):\n",
    "    #TODO\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1os-TdE1JS-"
   },
   "outputs": [],
   "source": [
    "def plot_epsilon(eps_evolution):\n",
    "    #TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ccu9YyPX1JS-"
   },
   "outputs": [],
   "source": [
    "#TODO mostrar gráficas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRc9q-LB1JS-"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentarios:</strong>\n",
    "#TODO\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxuZP8vS1JS-"
   },
   "source": [
    "Una vez entrenado el agente, nos interesa comprobar cómo de bien ha aprendido y si es capaz de conseguir superar el entorno. Para ello, recuperamos el modelo entrenado y dejamos que el agente tome acciones aleatorias según ese modelo y observamos su comportamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcbEPSqp1JS-"
   },
   "source": [
    "### 2.4 Test del agente.\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.25 ptos):</strong> Cargar el modelo entrenado y ejecutar el agente entrenado durante 505 episodios consecutivos en diferentes periodos aleatorios desde el año 2015 hasta el 2024. Calcula la suma de recompensas por cada ejecución. Para conseguir este punto, ejecuta:\n",
    "    <ul>\n",
    "        <li>Un gráfico con la suma de las recompensas respecto de los episodios, incluyendo el umbral de recompensa establecido</li>\n",
    "        <li>Almacena la recompensa media obtenida en las 100 partidas en la variable <code>mean_reward_dqn_test</code> y la última recompensa obtenida en el entrenamiento en <code>mean_reward_dqn_last</code>. También obten en cuantos escenarios se ha obtenido más de 252 dias positivos en el trading. </li>\n",
    "    </ul>\n",
    "Además, realiza la siguiente análisis con el modelo para el entorno utilizado en el entrenamiento durante :\n",
    " <ul>\n",
    " <li>Reproducir una partida completa del agente entrenado y mostrar el resultado final, incluyendo el valor total del portafolio al final del episodio.</li>\n",
    "        <li>Generar un fichero CSV que registre los resultados de las interacciones del agente con el mercado en cada episodio y muestra por pantalla las últimas 30 acciones.</li>\n",
    " </ul>\n",
    "<strong>Comenta todos los resultados obtenidos en este apartado. ¿A qué conclusiones podemos llegar? ¿Cómo podríamos mejorar el entrenamiento y qué implicaciones tendría?</strong>\n",
    "\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijUTlkn91JS_"
   },
   "outputs": [],
   "source": [
    "#Generar un fichero CSV que registre los resultados de las interacciones\n",
    "#del agente con el mercado en cada episodio y muestra por pantalla las últimas 30 acciones.\n",
    "file_path = 'stock_trading_agent_dqn.csv'\n",
    "#Reproducir una partida completa del agente entrenado y mostrar el resultado final,\n",
    "#incluyendo el valor total del portafolio al final del episodio.\n",
    "env = ''#TODO\n",
    "\n",
    "\n",
    "def read_csv_and_show_last_30(file_path):\n",
    "    try:\n",
    "        #TODO\n",
    "        pass\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"El archivo {file_path} no fue encontrado.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Se produjo un error al leer el archivo: {e}\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "read_csv_and_show_last_30(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Df3VnU3i1JS_"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generar calendario de trading usando días hábiles\n",
    "def generate_random_trading_dates(start_range, end_range, trading_days_target=505):\n",
    "    \"\"\"\n",
    "    Genera un par de fechas (start, end) que tengan exactamente trading_days_target días hábiles.\n",
    "    \"\"\"\n",
    "    start_date = pd.to_datetime(start_range)\n",
    "    end_date = pd.to_datetime(end_range)\n",
    "\n",
    "    while True:\n",
    "        # Seleccionar una fecha de inicio aleatoria\n",
    "        random_start = start_date + pd.DateOffset(days=random.randint(0, (end_date - start_date).days - trading_days_target))\n",
    "\n",
    "        # Generar un rango de fechas de trading usando solo los días hábiles\n",
    "        trading_days = pd.bdate_range(random_start, random_start + pd.DateOffset(days=2 * trading_days_target)).tolist()\n",
    "\n",
    "        # Filtrar las fechas para obtener exactamente el número de días objetivo\n",
    "        if len(trading_days) >= trading_days_target:\n",
    "            random_end = trading_days[trading_days_target - 1]  # Último día de trading en el rango deseado\n",
    "            return random_start.strftime(\"%Y-%m-%d\"), random_end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def test_model(ag, base_env, start_range, end_range, trading_days_target=505, win_days_target=252):\n",
    "    all_rewards = []\n",
    "    win_days_count = []\n",
    "\n",
    "    for i_episode in range(100):\n",
    "        # Generar nuevas fechas de inicio y fin aleatorias que cumplan con los días de trading deseados\n",
    "        start_date, end_date = generate_random_trading_dates(start_range, end_range, trading_days_target)\n",
    "\n",
    "        # Actualizar el entorno con las nuevas fechas\n",
    "        env = None  #TODO\n",
    "\n",
    "        #TODO\n",
    "\n",
    "        env.close()\n",
    "\n",
    "\n",
    "\n",
    "    return all_rewards,success_rate\n",
    "\n",
    "\n",
    "def plot_test(rewards, th):\n",
    "    pass\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nf-dQTJSNMg-"
   },
   "outputs": [],
   "source": [
    "mean_reward_dqn = 0 #TODO media de las 100 partidas de test\n",
    "mean_reward_dqn_last = 0 #TODO Mean Reward la última iteración del entrenamiento.\n",
    "success_rate = 0\n",
    "print(f\"La recompensa media obtenida por el agente DQN en las 100 partidas de test es: {mean_reward_dqn:.2f} puntos.\")\n",
    "print(f\"Porcentaje de episodios que lograron ganar al menos 252 días: {success_rate * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zivDev7J1JS_"
   },
   "source": [
    "\n",
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentarios:</strong>\n",
    "#TODO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLVL98bV1JS_"
   },
   "source": [
    "## 3. Agente Dueling DQN (1.5 ptos)\n",
    "\n",
    "En este apartado resolveremos el mismo entorno con las mismas características para el agente, pero usando una dueling DQN. Como en el caso anterior, primero definiremos el modelo de red neuronal, luego describiremos el comportamiento del agente, lo entrenaremos y, finalmente, testearemos el funcionamiento del agente entrenado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puwBgnP41JS_"
   },
   "source": [
    "### 3.1 Definición de la arquitectura  de la red neuronal\n",
    "\n",
    "\n",
    "El objetivo principal de las dueling DQN es \"ahorrarse\" el cálculo del valor de Q en aquéllos estados en los que es irrelevante la acción que se tome. Para ello se descompone la función Q en dos componentes:\n",
    "\n",
    "\n",
    "$$Q(s, a) = A(s, a) + V (s)$$\n",
    "\n",
    "\n",
    "Esta descomposición se realiza a nivel de la arquitectura de la red neuronal. Las primeras capas que teníamos en la DQN serán comunes, y luego la red se dividirá en dos partes separadas definidas por el resto de capas.\n",
    "\n",
    "\n",
    "La descomposición en sub-redes del modelo de la DQN implementada en el apartado anterior, será entonces:\n",
    "\n",
    "<ol> <li> Bloque común: </li> <ul> <li>Una primera capa completamente conectada de 256 neuronas y <code>bias = True</code>, con activación ReLU </li\n",
    "<li>Una primera capa completamente conectada de 128 neuronas y <code>bias = True</code>, con activación ReLU </li\n",
    "\n",
    "> </ul> <li>Para cada una de las subredes de ventaja A(s,a) y valor V(s):</li> <ul> <li>Una capa completamente conectada de 64 neuronas y <code>bias = True</code>, con activación ReLU </li> <li>Una última capa completamente conectada y <code>bias = True</code>. Esta será nuestra capa de salida y por tanto el número de neuronas de salida dependerá de si se trata de la red A(s,a), que tendrá tantas neuronas como dimensiones tenga el espacio de acciones, o si se trata de la red V(s), con un valor por estado.</li> </ul> </ol>\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGxOz5G21JS_"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 3.1 (0.5 ptos):</strong> Implementar la clase <code>duelingDQN()</code>. Inicializar las variables necesarias y definir el modelo de red neuronal indicado.\n",
    "\n",
    "    -----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se os proporciona el código pre-implementado. La implementación que se pide en el enunciado está indicada en los bloques <i>#TODO</i> y/o con variables igualadas a <i>None</i>.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltuTJg4w1JTA"
   },
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "\n",
    "class duelingDQN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, env, device=None, learning_rate=1e-3):\n",
    "\n",
    "        \"\"\"\n",
    "        Parámetros\n",
    "        ==========\n",
    "        n_inputs: tamaño del espacio de estados\n",
    "        n_outputs: tamaño del espacio de acciones\n",
    "        actions: array de acciones posibles\n",
    "        \"\"\"\n",
    "\n",
    "        ###################################\n",
    "        ####TODO: Inicializar variables####\n",
    "        super(duelingDQN, self).__init__()\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.n_inputs = #TODO\n",
    "        self.n_outputs = #TODO\n",
    "        self.actions = #TODO\n",
    "\n",
    "        ######\n",
    "\n",
    "        #######################################\n",
    "        ##TODO: Construcción de la red neuronal\n",
    "        # Red común\n",
    "        ##Construcción de la red neuronal\n",
    "\n",
    "        self.model_common = #TODO\n",
    "\n",
    "        # Subred de la función de Valor\n",
    "        self.fc_layer_inputs = self.feature_size()\n",
    "\n",
    "\n",
    "        self.advantage  = #TODO\n",
    "\n",
    "        # Recordad adaptarlas a cpu o gpu\n",
    "\n",
    "        # Subred de la Ventaja A(s,a)\n",
    "        self.value = #TODO\n",
    "\n",
    "        #######\n",
    "        #######################################\n",
    "        ##TODO: Inicializar el optimizador\n",
    "        self.optimizer = #TODO\n",
    "\n",
    "\n",
    "    #######################################\n",
    "    #####TODO: función forward#############\n",
    "    def forward(self, state):\n",
    "        # Conexión entre capas de la red común\n",
    "        common_out = #TODO\n",
    "\n",
    "        # Conexión entre capas de la Subred de Valor\n",
    "        advantage = #TODO\n",
    "\n",
    "        # Conexión entre capas de la Subred de Ventaja\n",
    "        value = #TODO\n",
    "\n",
    "\n",
    "        ## Agregar las dos subredes:\n",
    "        # Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\n",
    "        action = #TODO\n",
    "\n",
    "        return action\n",
    "    #######\n",
    "\n",
    "\n",
    "\n",
    "    ### MéTODO e-greedy\n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            qvals = self.get_qvals(state)\n",
    "            action = torch.max(qvals, dim=-1)[1].item()\n",
    "        return action\n",
    "\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        if type(state) is tuple:\n",
    "            state = np.array([np.ravel(s) for s in state])\n",
    "        state_t = torch.FloatTensor(state).to(self.device)\n",
    "        return self.forward(state_t)\n",
    "\n",
    "    def feature_size(self):\n",
    "        dummy_input = torch.zeros(1, *env.observation_space.shape).to(self.device)\n",
    "        return self.model_common(autograd.Variable(dummy_input)).view(1, -1).size(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-QAliFGBBy7"
   },
   "source": [
    "Para el buffer de repetición de experiencias podemos usar exactamente la misma clase experienceReplayBuffer descrita en el apartado anterior de la DQN.\n",
    "\n",
    "\n",
    "### 3.2 Definición del agente\n",
    "\n",
    "La diferencia entre la DQN y la dueling DQN se centra, como hemos visto, en la definición de la arquitectura de la red. Pero el proceso de aprendizaje y actualización es exactamente el mismo. Así, podemos recuperar la clase implementada en el apartado anterior, DQNAgent() y reutilizarla aquí bajo el nombre de duelingDQNAgent(). Lo único que deberemos hacer es añadir el optimizador entre las variables a declarar y adaptar la función de pérdida al formato Functional de pytorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXnacQ_e1JTA"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 3.2 (0.25 pto):</strong> Implementar la clase <code>duelingDQNAgent()</code> como la <code>DQNAgent()</code>\n",
    "<p>\n",
    "</p>\n",
    "De nuevo, durante el proceso se deben almacenar (*):\n",
    "    <ul>\n",
    "        <li>Las recompensas obtenidas en cada paso del entrenamiento</li>\n",
    "        <li>Las recompensas medias de los 100 episodios anteriores</li>\n",
    "        <li>La pérdida durante el entrenamiento</li>\n",
    "        <li>La evolución de epsilon a lo largo del entrenamiento</li>\n",
    "    </ul>\n",
    "\n",
    "    -----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se os proporciona el código pre-implementado. La implementación que se pide en el enunciado está indicada en los bloques <i>#TODO</i> y/o con variables igualadas a <i>None</i>, salvo (*) en qué momento almacenar las variables que se indican."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0FuC3kM1JTA"
   },
   "outputs": [],
   "source": [
    "class duelingDQNAgent:\n",
    "\n",
    "    def __init__(self, env, main_network, buffer, reward_threshold, epsilon=0.1, eps_decay=0.99, batch_size=32, device= None):\n",
    "        \"\"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        env: entorno\n",
    "        target_network: clase con la red neuronal diseñada\n",
    "        target_network: red objetivo\n",
    "        buffer: clase con el buffer de repetición de experiencias\n",
    "        epsilon: epsilon\n",
    "        eps_decay: epsilon decay\n",
    "        batch_size: batch size\n",
    "        nblock: bloque de los X últimos episodios de los que se calculará la media de recompensa\n",
    "        reward_threshold: umbral de recompensa definido en el entorno\n",
    "        \"\"\"\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        ###############################################################\n",
    "        #####TODO 1: inicialitzar variables######\n",
    "\n",
    "        self.env = None #TODO\n",
    "        self.main_network =None #TODO\n",
    "        self.target_network =None #TODO  # red objetivo (copia de la principal)\n",
    "        self.buffer =None #TODO\n",
    "        self.epsilon =None #TODO\n",
    "        self.eps_decay =None #TODO\n",
    "        self.batch_size =None #TODO\n",
    "        self.nblock =None #TODO # bloque de los X últimos episodios de los que se calculará la media de recompensa\n",
    "        self.reward_threshold = reward_threshold #umbral de recompensa definido en el entorno\n",
    "\n",
    "        self.initialize()\n",
    "\n",
    "\n",
    "    ###############################################################\n",
    "    #####TODO 2: inicialitzar variables extra que se necessiten######\n",
    "    def initialize(self):\n",
    "        pass\n",
    "        #TODO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #################################################################################\n",
    "    ######TODO 3:  Tomar nueva acción ###############################################\n",
    "    def take_step(self, eps, mode='train'):\n",
    "        if mode == 'explore':\n",
    "            action = None  # TODO acción aleatoria en el burn-in\n",
    "        else:\n",
    "           action = None  # TODO  acción a partir del valor de Q (elección de la acción con mejor Q)\n",
    "            self.step_count += 1\n",
    "\n",
    "        #TODO: Realización de la acción y obtención del nuevo estado y la recompensa\n",
    "\n",
    "        #TODO: resetear entorno 'if done'\n",
    "        if done:\n",
    "            pass #TODO\n",
    "        return done\n",
    "\n",
    "\n",
    "    ## Entrenamiento\n",
    "    def train(self, gamma=0.99, max_episodes=50000,\n",
    "              batch_size=32,\n",
    "              dnn_update_frequency=4,\n",
    "              dnn_sync_frequency=2000, min_episodios=250):\n",
    "        self.gamma = gamma\n",
    "        # Rellenamos el buffer con N experiencias aleatorias ()\n",
    "        print(\"Filling replay buffer...\")\n",
    "        while self.buffer.burn_in_capacity() < 1:\n",
    "            self.take_step(self.epsilon, mode='explore')\n",
    "\n",
    "        episode = 0\n",
    "        training = True\n",
    "        print(\"Training...\")\n",
    "        while training:\n",
    "            self.state0 = self.env.reset()[0]\n",
    "            self.total_reward = 0\n",
    "            gamedone = False\n",
    "            while gamedone == False:\n",
    "                # El agente toma una acción\n",
    "                gamedone = self.take_step(self.epsilon, mode='train')\n",
    "\n",
    "                #################################################################################\n",
    "                #####TODO 4: Actualizar la red principal según la frecuencia establecida  #######\n",
    "\n",
    "                ########################################################################################\n",
    "                ###TODO6: Sincronizar red principal y red objetivo según la frecuencia establecida#####\n",
    "\n",
    "\n",
    "                if gamedone:\n",
    "                    episode += 1\n",
    "                    ##################################################################\n",
    "                    ########TODO: Almacenar epsilon, training rewards i loss#######\n",
    "\n",
    "                    ####\n",
    "                    self.update_loss = []\n",
    "\n",
    "\n",
    "                    #######################################################################################\n",
    "                    ###TODO 7: calcular la media de recompensa de los últimos X episodios, y almacenar#####\n",
    "                    mean_rewards = None\n",
    "                    ###\n",
    "\n",
    "                    print(\"\\rEpisode {:d} Mean Rewards {:.2f} Epsilon {}\\t\\t\".format(\n",
    "                        episode, mean_rewards, self.epsilon), end=\"\")\n",
    "\n",
    "                    # Comprobar si se ha llegado al máximo de episodios\n",
    "                    if episode >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        break\n",
    "\n",
    "                    # Termina el juego si la media de recompensas ha llegado al umbral fijado para este juego\n",
    "                    # y se ha entrenado un mínimo de episodios\n",
    "                    if mean_rewards >= self.reward_threshold and min_episodios <  episode:\n",
    "                        training = False\n",
    "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                            episode))\n",
    "                        break\n",
    "\n",
    "                    #################################################################################\n",
    "                    ######TODO 8: Actualizar epsilon ########\n",
    "                    self.epsilon = None\n",
    "\n",
    "     ## Cálculo de la pérdida\n",
    "    def calculate_loss(self, batch):\n",
    "        # Separamos las variables de la experiencia y las convertimos a tensores\n",
    "        states, actions, rewards, dones, next_states = [i for i in batch]\n",
    "        rewards_vals = torch.FloatTensor(rewards).to(self.device).reshape(-1,1)\n",
    "        actions_vals = torch.LongTensor(np.array(actions)).reshape(-1,1).to(self.device)\n",
    "        dones_t = torch.ByteTensor(dones).to(self.device)\n",
    "\n",
    "        # Obtenemos los valores de Q de la red principal\n",
    "        qvals = torch.gather(self.main_network.get_qvals(states).to(self.device), 1, actions_vals)\n",
    "\n",
    "        #update#\n",
    "        next_actions = torch.max(self.main_network.get_qvals(next_states).to(self.device), dim=-1)[1]\n",
    "        next_actions_vals = next_actions.reshape(-1, 1).to(self.device)\n",
    "\n",
    "\n",
    "        # Obtenemos los valores de Q de la red objetivo\n",
    "        target_qvals = self.target_network.get_qvals(next_states).to(self.device)\n",
    "        qvals_next = torch.gather(target_qvals, 1, next_actions_vals).detach()\n",
    "        #####\n",
    "        qvals_next[dones_t.bool()] = 0\n",
    "        #qvals_next[dones_t] = 0 # 0 en estados terminales\n",
    "        # Calculamos ecuación de Bellman\n",
    "        expected_qvals = None\n",
    "        #Función Loss #####\n",
    "        loss = torch.nn.MSELoss()(qvals, expected_qvals.reshape(-1,1))\n",
    "        #######\n",
    "        return loss\n",
    "\n",
    "    def update(self):\n",
    "        self.main_network.optimizer.zero_grad()  # eliminamos cualquier gradiente pasado\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size) # seleccionamos un conjunto del buffer\n",
    "        loss = self.calculate_loss(batch) # calculamos la pérdida\n",
    "        loss.backward() # hacemos la diferencia para obtener los gradientes\n",
    "        self.main_network.optimizer.step() # aplicamos los gradientes a la red neuronal\n",
    "        # Guardamos los valores de pérdida\n",
    "        if self.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhxbHDHLBYYU"
   },
   "source": [
    "### 3.3 Entrenamiento del Modelo\n",
    "\n",
    "A continuación entrenaremos el modelo dueling DQN con los mismos hiperparámetros con los que entrenamos la DQN.\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 3.3 (0.25 ptos):</strong> Cargar el modelo de red neuronal y entrenar el agente con los mismos hiperparámetros usados para la DQN\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzDK0jVGBnyC"
   },
   "outputs": [],
   "source": [
    "#TODO Tiempo ejecución 69 mintuos en google colaboratory con GPU.\n",
    "#resultado esperado alrededor de 180-200 puntos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlGKaJwyB5ci"
   },
   "source": [
    "### 3.4 Análisis del entrenamiento\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 3.4 (0.25 ptos):</strong> Mostrar los mismos gráficos que con la DQN:\n",
    "    <ol>\n",
    "        <li>Recompensas obtenidas a lo largo del entrenamieno y la evolución de las recompensas medias cada 100 episodios, junto con el umbral de recompensa establecido por el entorno</li>\n",
    "        <li>Pérdida durante el entrenamiento</li>\n",
    "        <li>Evolución de epsilon a lo largo del entrenamiento</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHoR0XY71JTA"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DyLBp4p1JTB"
   },
   "source": [
    "### 3.5 Test del agente.\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.25 ptos):</strong> Cargar el modelo entrenado y ejecutar el agente entrenado durante 505 episodios consecutivos en diferentes periodos aleatorios desde el año 2015 hasta el 2024. Calcula la suma de recompensas por cada ejecución. Para conseguir este punto, ejecuta:\n",
    "    <ul>\n",
    "        <li>Un gráfico con la suma de las recompensas respecto de los episodios, incluyendo el umbral de recompensa establecido</li>\n",
    "        <li>Almacena la recompensa media obtenida en las 100 partidas en la variable <code>mean_reward_agentduelingDQN</code> y la última recompensa obtenida en el entrenamiento en <code>mean_reward_agentduelingDQN_last</code></li>\n",
    "    </ul>\n",
    "Además, realiza la siguiente análisis con el modelo para el entorno utilizado en el entrenamiento durante :\n",
    " <ul>\n",
    " <li>Reproducir una partida completa del agente entrenado y mostrar el resultado final, incluyendo el valor total del portafolio al final del episodio.</li>\n",
    "        <li>Generar un fichero CSV que registre los resultados de las interacciones del agente con el mercado en cada episodio y muestra por pantalla las últimas 30 acciones.</li>\n",
    " </ul>\n",
    "<strong>Comenta TODOs los resultados obtenidos en este apartado. ¿A qué conclusiones podemos llegar? ¿Cómo podríamos mejorar el entrenamiento y qué implicaciones tendría?</strong>\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQSAsiD81JTB"
   },
   "outputs": [],
   "source": [
    "file_path =  \"stock_trading_agent_ddqn.csv\"\n",
    "env = None #TODO\n",
    "\n",
    "\n",
    "\n",
    "mean_reward_agentduelingDQN = 0 #TODO\n",
    "mean_reward_agentduelingDQN_last = 0 #TODO\n",
    "print(f\"La recompensa media obtenida por el agente DQN en las 100 partidas de test es: {mean_reward_agentduelingDQN:.2f} puntos.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETW8_5TL1JTD"
   },
   "source": [
    "\n",
    "## 4.Comparación de los resultados (1.5 pto.)\n",
    "\n",
    "Ahora vamos a comparar los resultados, si has seguido todas las indicaciones, habrás almacenado métricas bastante interesantes que te permitirán interpretar los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wf3qjvYV1JTD"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define los datos de la tabla\n",
    "data = [\n",
    "    [\"DQN\", mean_reward_dqn_last, mean_reward_dqn, time_dqn],\n",
    "    [\"Dueling DQN\", mean_reward_agentduelingDQN_last, mean_reward_agentduelingDQN, time_ddqn],\n",
    "]\n",
    "\n",
    "# Define los encabezados de la tabla\n",
    "headers = [\"Agente\", \"Media Reward de Entrenamiento\", \"Media test con 100 Partidas Aleatorias\" , \"Tiempo entrenamiento.\"]\n",
    "\n",
    "# Imprime la tabla\n",
    "table = tabulate(data, headers, tablefmt=\"pipe\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zo0jZp-h1JTD"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (1.5 ptos):</strong>\n",
    "\n",
    "Comentar los resultados obtenidos. ¿Qué agente a obtenido mejor resultados? Justificalo.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsEGnkks1JTD"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentario:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cI1eaU4a1JTD"
   },
   "source": [
    "## 5. Compara los agentes en otro entorno.  (2 ptos)\n",
    "\n",
    "En esta parte de la PEC vamos a comparar cómo se desenvuelven dichos agentes en otro entorno diferente implementado por un tercero.\n",
    "\n",
    "Uno de los beneficios de haber utilizado Gymnasium es que podemos rápidamente utilizar nuestros algoritmos en todo aquel entorno que comparta la misma interfaz. Un ejemplo puede se [Cart Pole](https://gymnasium.farama.org/environments/classic_control/cart_pole/), un entorno clásico que consiste en un carrito sobre el que se apoya una barra vertical. El objetivo es mantener la barra en equilibrio evitando que caiga, aplicando pequeñas fuerzas al carrito hacia la derecha o hacia la izquierda. Las únicas acciones posibles son estas fuerzas, que permiten al algoritmo aprender a mantener el equilibrio de la barra en la posición correcta.\n",
    "\n",
    "![cartpole](https://gymnasium.farama.org/_images/cart_pole.gif)\n",
    "\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (2 ptos):</strong>\n",
    "\n",
    "Ejecuta el agente DQN y Dueling DQN en el nuevo entorno. Una vez lo hayar realizado, implementar una tabla como la mostrar en el ejercico anterior y análiza los resultados. ¿Contínua siendo el mismo agente el que mejor resultado ha obtenido?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7Tu93R01JTD"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Configuración de hiperparámetros para dqn.\n",
    "lr = None             # Velocidad de aprendizaje ajustada para mejor convergencia\n",
    "MEMORY_SIZE = None     # Capacidad de memoria reducida, suficiente para un entorno simple como CartPole\n",
    "MAX_EPISODES = 5000     # Número máximo de episodios reducido, ya que CartPole es un problema más sencillo\n",
    "EPSILON = None             # Valor inicial de epsilon (alta exploración inicial)\n",
    "EPSILON_DECAY = None    # Decaimiento de epsilon ajustado para un descenso más gradual\n",
    "GAMMA = None           # Factor de descuento gamma l\n",
    "BATCH_SIZE = None         # Tamaño del lote para el entrenamiento\n",
    "BURN_IN = None           # Episodios iniciales para llenar el buffer de experiencia antes de entrenar\n",
    "DNN_UPD = 1             # Frecuencia de actualización de la red neuronal (cada paso)\n",
    "DNN_SYNC = 1000         # Frecuencia de sincronización de pesos\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFeqn26fpm3g"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def test_model(ag, env):\n",
    "    all_rewards = []\n",
    "    # Usamos tqdm para el bucle de episodios\n",
    "    for i_episode in tqdm(range(100), desc=\"Progreso de episodios\"):\n",
    "        #rodo\n",
    "        pass\n",
    "\n",
    "    return all_rewards\n",
    "\n",
    "\n",
    "\n",
    "def plot_test(rewards, th):\n",
    "    #TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RANwmV4WpuM6"
   },
   "outputs": [],
   "source": [
    "#utiliza las funciones anteriores para imprimir la evolución de los agenetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMW_Jfvi1JTE"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Muestras los datos de los entrenamientos de cada agente.\n",
    "data = [\n",
    "    [\"DQN\", mean_reward_dqn_last, mean_reward_dqn, time_dqn],\n",
    "    [\"Dueling DQN\", mean_reward_agentduelingDQN_last, mean_reward_agentduelingDQN, time_ddqn],\n",
    "]\n",
    "\n",
    "# Define los encabezados de la tabla\n",
    "headers = [\"Agente\", \"Media Reward de Entrenamiento\", \"Media test con 100 Partidas Aleatorias\" , \"Tiempo entrenamiento.\"]\n",
    "\n",
    "# Imprime la tabla\n",
    "table = tabulate(data, headers, tablefmt=\"pipe\")\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
