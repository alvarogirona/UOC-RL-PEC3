{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATAcq7bQyNxe"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"https://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/llibre-estil/logo-UOC-masterbrand-vertical.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.883 · Aprendizaje por refuerzo</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2023-1 · Máster universitario en Ciencia de datos (<i>Data science</i>)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-bottom: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "# PEC2: Deep Reinforcement Learning\n",
    "\n",
    "\n",
    "En esta práctica se implementarán - modelos de DRL en dos entornos diferentes, con el objetivo de analizar distintas formas de aprendizaje de un agente y estudiar su rendimiento. El agente será entrenado con los métodos:\n",
    "\n",
    "<ol>\n",
    "    <li>DQN</li>\n",
    "    <li>Dueling DQN</li>\n",
    "</ol>\n",
    "  \n",
    "\n",
    "**Importante: La entrega debe hacerse en formato notebook y en formato html donde se vea el código y los resultados y comentarios de cada ejercicio. Para exportar el notebook a html puede hacerse desde el menú File  →  Download as  →  HTML.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZXQSvJSyNxk"
   },
   "source": [
    "## 0. Contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zan-tCv_zJXF"
   },
   "source": [
    "El aprendizaje por refuerzo es un campo de la inteligencia artificial que busca desarrollar sistemas capaces de aprender y tomar decisiones autónomas a través de la interacción con su entorno. A lo largo de los años, este enfoque ha demostrado su capacidad para abordar una amplia gama de aplicaciones, desde juegos de mesa hasta robótica y gestión de recursos. Sin embargo, una de las cuestiones más desafiantes en el aprendizaje por refuerzo es la creación de entornos de simulación adecuados que reflejen fielmente el contexto de la aplicación deseada.\n",
    "\n",
    "En este contexto, esta PEC tiene como objetivo desarrollar un nuevo entorno de simulación que permita la investigación y experimentación con diferentes agentes de trading. Este entorno estará diseñado específicamente para abordar un problema ficticio de inversión y gestión de un portafolio en el mercado de valores, en el que un agente debe aprender a tomar decisiones óptimas de compra, venta o mantenimiento de acciones. El objetivo del agente será maximizar las ganancias a lo largo del tiempo mediante estrategias basadas en el aprendizaje por refuerzo.\n",
    "\n",
    "Para ello, se utilizará el entorno adaptado a las especificaciones de Gymnasium (https://gymnasium.farama.org/index.html), que permite la creación de entornos personalizados para el aprendizaje por refuerzo. Este entorno simulará el comportamiento dinámico de un mercado financiero, con fluctuaciones en los precios de las acciones y eventos de mercado que afecten las decisiones del agente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzjteFjb1JS1"
   },
   "source": [
    "## 1. Creación de un entorno en Gym (3 ptos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1J3ZQQ2zJXh"
   },
   "source": [
    "En este ejercicio diseñaremos un entorno sencillo siguiendo el esquema de los entornos de <code>Gymnasium</code>, y trataremos de resolverlo.\n",
    "\n",
    "Los entornos de <code>Gymnasium</code> suelen tener la siguiente estructura:\n",
    "\n",
    "```\n",
    "class FooEnv(gym.Env):\n",
    "  metadata = {'render.modes': ['human']}\n",
    "\n",
    "  def __init__(self):\n",
    "    ...\n",
    "  def step(self, action):\n",
    "    ...\n",
    "    return new_state, reward, terminated, truncated, info\n",
    "\n",
    "  def reset(self):\n",
    "    ...\n",
    "    return observation, info\n",
    "\n",
    "  def render(self, mode='human', close=False):\n",
    "    ...\n",
    "\n",
    " ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U1O0HWGzJXh"
   },
   "source": [
    "El primer paso será instalar las librerías necesarias para abordar la PEC:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TYp6OAfE1JS2",
    "ExecuteTime": {
     "end_time": "2024-12-04T14:42:48.785529Z",
     "start_time": "2024-12-04T14:42:36.180283Z"
    }
   },
   "source": [
    "!pip install gymnasium\n",
    "!pip install torch\n",
    "\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install tensorboard\n",
    "!pip install tdqm\n",
    "!pip install tabulate\n",
    "!pip install yfinance\n",
    "!pip install pandas"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in ./venv/lib/python3.11/site-packages (1.0.0)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./venv/lib/python3.11/site-packages (from gymnasium) (2.1.3)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./venv/lib/python3.11/site-packages (from gymnasium) (3.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./venv/lib/python3.11/site-packages (from gymnasium) (4.12.2)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./venv/lib/python3.11/site-packages (from gymnasium) (0.0.4)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.11/site-packages (2.2.2)\r\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from torch) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.11/site-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.11/site-packages (from torch) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.11/site-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.11/site-packages (from torch) (2024.10.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.11/site-packages (3.9.3)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.11/site-packages (from matplotlib) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.11/site-packages (from matplotlib) (4.55.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.11/site-packages (from matplotlib) (1.4.7)\r\n",
      "Requirement already satisfied: numpy>=1.23 in ./venv/lib/python3.11/site-packages (from matplotlib) (2.1.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.11/site-packages (from matplotlib) (24.2)\r\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.11/site-packages (from matplotlib) (11.0.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.11/site-packages (from matplotlib) (3.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.11/site-packages (2.1.3)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: tensorboard in ./venv/lib/python3.11/site-packages (2.18.0)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in ./venv/lib/python3.11/site-packages (from tensorboard) (2.1.0)\r\n",
      "Requirement already satisfied: grpcio>=1.48.2 in ./venv/lib/python3.11/site-packages (from tensorboard) (1.68.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.11/site-packages (from tensorboard) (3.7)\r\n",
      "Requirement already satisfied: numpy>=1.12.0 in ./venv/lib/python3.11/site-packages (from tensorboard) (2.1.3)\r\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.11/site-packages (from tensorboard) (24.2)\r\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in ./venv/lib/python3.11/site-packages (from tensorboard) (5.29.0)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./venv/lib/python3.11/site-packages (from tensorboard) (68.2.0)\r\n",
      "Requirement already satisfied: six>1.9 in ./venv/lib/python3.11/site-packages (from tensorboard) (1.16.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./venv/lib/python3.11/site-packages (from tensorboard) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./venv/lib/python3.11/site-packages (from tensorboard) (3.1.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venv/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: tdqm in ./venv/lib/python3.11/site-packages (0.0.1)\r\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.11/site-packages (from tdqm) (4.67.1)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: tabulate in ./venv/lib/python3.11/site-packages (0.9.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: yfinance in ./venv/lib/python3.11/site-packages (0.2.50)\r\n",
      "Requirement already satisfied: pandas>=1.3.0 in ./venv/lib/python3.11/site-packages (from yfinance) (2.2.3)\r\n",
      "Requirement already satisfied: numpy>=1.16.5 in ./venv/lib/python3.11/site-packages (from yfinance) (2.1.3)\r\n",
      "Requirement already satisfied: requests>=2.31 in ./venv/lib/python3.11/site-packages (from yfinance) (2.32.3)\r\n",
      "Requirement already satisfied: multitasking>=0.0.7 in ./venv/lib/python3.11/site-packages (from yfinance) (0.0.11)\r\n",
      "Requirement already satisfied: lxml>=4.9.1 in ./venv/lib/python3.11/site-packages (from yfinance) (5.3.0)\r\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in ./venv/lib/python3.11/site-packages (from yfinance) (4.3.6)\r\n",
      "Requirement already satisfied: pytz>=2022.5 in ./venv/lib/python3.11/site-packages (from yfinance) (2024.2)\r\n",
      "Requirement already satisfied: frozendict>=2.3.4 in ./venv/lib/python3.11/site-packages (from yfinance) (2.4.6)\r\n",
      "Requirement already satisfied: peewee>=3.16.2 in ./venv/lib/python3.11/site-packages (from yfinance) (3.17.8)\r\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in ./venv/lib/python3.11/site-packages (from yfinance) (4.12.3)\r\n",
      "Requirement already satisfied: html5lib>=1.1 in ./venv/lib/python3.11/site-packages (from yfinance) (1.1)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv/lib/python3.11/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\r\n",
      "Requirement already satisfied: six>=1.9 in ./venv/lib/python3.11/site-packages (from html5lib>=1.1->yfinance) (1.16.0)\r\n",
      "Requirement already satisfied: webencodings in ./venv/lib/python3.11/site-packages (from html5lib>=1.1->yfinance) (0.5.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.11/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.11/site-packages (from pandas>=1.3.0->yfinance) (2024.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests>=2.31->yfinance) (3.4.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests>=2.31->yfinance) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests>=2.31->yfinance) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests>=2.31->yfinance) (2024.8.30)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.11/site-packages (2.2.3)\r\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./venv/lib/python3.11/site-packages (from pandas) (2.1.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.11/site-packages (from pandas) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.11/site-packages (from pandas) (2024.2)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsmRtf2Z1JS3"
   },
   "source": [
    "y las importamos:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yuHxDz3ZXawz",
    "ExecuteTime": {
     "end_time": "2024-12-04T14:43:21.641252Z",
     "start_time": "2024-12-04T14:43:21.637491Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "from collections import namedtuple, deque\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "print(\"Gym Version:\", gym.__version__)  # 0.28.1\n",
    "print(\"Gym Version:\", torch.__version__)  # 0.28.1"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym Version: 1.0.0\n",
      "Gym Version: 2.2.2\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVVI0tZnp9v2"
   },
   "source": [
    "\n",
    "###  1.1 Entorno de Simulación para Trading Automático en el Mercado de Valores\n",
    "\n",
    "***¡Enhorabuena!*** Una firma de inversión ha decidido contrataros para desarrollar un sistema de trading automático para sus operaciones en el mercado de valores. Para ello, os piden que diseñéis un entorno de simulación que permita entrenar un agente capaz de tomar decisiones de compra, venta o mantener posiciones sobre una acción determinada, maximizando las ganancias a lo largo del tiempo. El entorno debe cumplir las siguientes especificaciones:\n",
    "\n",
    "<ul>\n",
    "  <li>El entorno debe llamarse <code>StockMarketEnv</code>\n",
    "  </li>\n",
    "  <li>El entorno debe heredar de la clase <code>gym.Env</code>\n",
    "  </li>\n",
    "  <li>El precio inicial de la acción estará basado en datos históricos, obtenidos a partir de una consulta a Yahoo Finance.</li>\n",
    "  <li>El balance inicial del agente será de 10,000 dolares, el cual puede ser utilizado para comprar acciones.</li>\n",
    "  <li>El agente puede realizar las siguientes acciones: </li>\n",
    "  \n",
    "  <ul>\n",
    "    <li>0 -> Mantener (no se realizan operaciones)</li>\n",
    "    <li>1 -> Comprar (se compra todas las acciones posibles al precio actual)</li>\n",
    "    <li>2 -> Vender (se vende todas las acciones disponibles al precio actual)</li>\n",
    "  </ul>\n",
    "  <li>El sistema de recompensas será el siguiente:</li>\n",
    "  <ul>\n",
    "    <li>El agente recibe una recompensa +1 si el valor neto de su portafolio (balance_actual + balance_anterior) aumenta respecto al paso anterior.</li>\n",
    "    <li>El agente recibe una recompensa de +1 si el valor neto de su portafolio (balance_actual) se mantiene igual, no posee ninguna acción y el valor de las acciones disminuye. Dicha comprobación no se realiza el primer dia de trading.</li>\n",
    "    <li>El agente recibe una recompensa de -1 si el valor neto de su portafolio (balance_actual) se mantiene igual, no posee ninguna acción y el valor de las acciones aumenta con respecto al día anterior. Dicha comprobación no se realiza el primer dia de trading. </li>\n",
    "    <li>Si el valor neto disminuye con respecto al día anterior, el agente recibe una recompensa -1.</li>\n",
    "    <li>En otros casos recibe una puntuación de 0.</li>\n",
    "  </ul>\n",
    "  <li>El entorno tendrá una duración por defecto para el entrenamiento 2019-01-01 hasta 2021-01-01 para el entrenamiento.</li>\n",
    "  <li>El entorno finalizará si el valor neto del portafolio cae por debajo del 85% del balance inicial (es decir, 8,500 dolares ).</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "El objetivo de este entorno es que el agente aprenda a tomar decisiones óptimas de compra y venta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EulU8U8F1JS4"
   },
   "source": [
    "\n",
    "![Imagen de Stock Market](https://media1.tenor.com/m/wWvt6qEQB8EAAAAd/kah.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWhwescU1JS4"
   },
   "source": [
    "#### 1.1.1 Implementación de los indicadores económicos\n",
    "\n",
    "El primer paso es implementar dos funciones llamadas `calculate_rsi` y `calculate_ema` que calcularán diferentes indicadores técnicos utilizados en el análisis de mercados financieros. Estos indicadores ayudarán a los agentes de trading a tomar decisiones basadas en patrones y tendencias del mercado.\n",
    "\n",
    "A continuación, se explica en qué consisten estas métricas:\n",
    "\n",
    "*  RSI (Índice de Fuerza Relativa): Calcula el RSI utilizando el cambio de precio durante una ventana de tiempo especificada. Este indicador muestra si un activo está sobrecomprado o sobrevendido. Podéis ver una explicación más detallada en https://es.wikipedia.org/wiki/%C3%8Dndice_de_fuerza_relativa\n",
    "*  EMA (Media Móvil Exponencial): Calcula la EMA, que es una versión ponderada de la media móvil que da más peso a los precios recientes. Podéis ver una explicación más detallada en https://es.tradingview.com/support/solutions/43000592270/\n",
    "\n",
    "Las funciones toman algunos de estos argumentos:\n",
    "\n",
    "* data: Los datos históricos de precios de las acciones, generalmente en formato de series temporales. En este ejemplo utilizaremos los precios de cierre diarios.\n",
    "* window (opcional): El número de periodos a utilizar para los cálculos de indicadores. Por defecto, se asume un valor de 14 para el RSI y la EMA.\n",
    "\n",
    "A continuación, se muestran las funciónes:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KxrX5J9U1JS4",
    "ExecuteTime": {
     "end_time": "2024-12-04T15:14:13.610443Z",
     "start_time": "2024-12-04T15:14:13.607082Z"
    }
   },
   "source": [
    "def calculate_rsi(data, window=14):\n",
    "    delta = data.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi.fillna(50).squeeze()\n",
    "\n",
    "def calculate_ema(data, window=14):\n",
    "    return data.ewm(span=window, adjust=False).mean().squeeze()\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfJKeOH61JS5"
   },
   "source": [
    "Con estas funciones, los agentes podrán utilizar información técnica clave sobre las acciones en el mercado para tomar decisiones de trading más informadas.\n",
    "\n",
    "Ahora bien, necesitamos el valor de las acciones. Para este proyecto, utilizamos la librería yfinance, que permite la obtención de datos históricos de activos financieros de manera sencilla. El siguiente fragmento de código descarga los datos del ETF SPY (un fondo que sigue al índice S&P 500) desde el 1 de enero de 2021 hasta el 1 de enero de 2022:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zviO1kyU1JS5",
    "ExecuteTime": {
     "end_time": "2024-12-04T15:15:21.097774Z",
     "start_time": "2024-12-04T15:15:21.075143Z"
    }
   },
   "source": [
    "data = yf.download(\"SPY\", start=\"2021-01-01\", end=\"2022-01-01\")\n",
    "\n",
    "print(data)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price        Adj Close       Close        High         Low        Open  \\\n",
      "Ticker             SPY         SPY         SPY         SPY         SPY   \n",
      "Date                                                                     \n",
      "2021-01-04  349.471680  368.790009  375.450012  364.820007  375.309998   \n",
      "2021-01-05  351.878632  371.329987  372.500000  368.049988  368.100006   \n",
      "2021-01-06  353.982269  373.549988  376.980011  369.119995  369.709991   \n",
      "2021-01-07  359.241608  379.100006  379.899994  375.910004  376.100006   \n",
      "2021-01-08  361.288452  381.260010  381.489990  377.100006  380.589996   \n",
      "...                ...         ...         ...         ...         ...   \n",
      "2021-12-27  458.288177  477.260010  477.309998  472.010010  472.059998   \n",
      "2021-12-28  457.913696  476.869995  478.809998  476.059998  477.720001   \n",
      "2021-12-29  458.499451  477.480011  478.559998  475.920013  476.980011   \n",
      "2021-12-30  457.231903  476.160004  479.000000  475.670013  477.929993   \n",
      "2021-12-31  456.079590  474.959991  476.859985  474.670013  475.640015   \n",
      "\n",
      "Price          Volume  \n",
      "Ticker            SPY  \n",
      "Date                   \n",
      "2021-01-04  110210800  \n",
      "2021-01-05   66426200  \n",
      "2021-01-06  107997700  \n",
      "2021-01-07   68766800  \n",
      "2021-01-08   71677200  \n",
      "...               ...  \n",
      "2021-12-27   56808600  \n",
      "2021-12-28   47274600  \n",
      "2021-12-29   54503000  \n",
      "2021-12-30   55329000  \n",
      "2021-12-31   65237400  \n",
      "\n",
      "[252 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agW2zE6Xp9v4"
   },
   "source": [
    "El resultado es un DataFrame que contiene la siguiente información para cada día del rango de fechas:\n",
    "\n",
    "- **Open**: Precio de apertura del activo.\n",
    "- **High**: Precio máximo del activo en el día.\n",
    "- **Low**: Precio mínimo del activo en el día.\n",
    "- **Close**: Precio de cierre del activo.\n",
    "- **Adj Close**: Precio ajustado que tiene en cuenta dividendos y splits.\n",
    "- **Volume**: Número de acciones negociadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxDFUMS2p9v5"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Ejercicio 1 (0.25 ptos):</strong>\n",
    "    Utiliza los datos históricos del mercado financiero, descargados mediante la función <code>yfinance.download</code>, y aplica los indicadores técnicos proporcionados: <code>calculate_rsi</code> y  <code>calculate_ema</code>\n",
    "    A continuación, realiza las siguientes tareas:\n",
    "    <ul>\n",
    "        <li>Descarga los datos históricos de SPY para el año 2021.</li>\n",
    "        <li>Calcula el RSI para los precios de cierre durante el período.</li>\n",
    "        <li>Calcula la media móvil exponencial (EMA) para el mismo período  con los precios de cierre.</li>\n",
    "        <li>Imprime el último valor de los cálculos de cada indicador (RSI y EMA ) para verificar que se han generado correctamente sin errores. Los valores obtenidos deberían ser RSI: 53.765164 i EMA: 470.690088 (el número de decimales puede variar).</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gjYAIycip9v5",
    "ExecuteTime": {
     "end_time": "2024-12-04T15:36:45.219918Z",
     "start_time": "2024-12-04T15:36:45.201061Z"
    }
   },
   "source": [
    "data_2021 = yf.download(\"SPY\", start=\"2021-01-01\", end=\"2022-01-01\")\n",
    "rsi_2021_close = calculate_rsi(data_2021[\"Close\"])\n",
    "ema_2021_close = calculate_ema(data_2021[\"Close\"])\n",
    "\n",
    "print(\"RSI 2021 close:\")\n",
    "print(rsi_2021_close[-1])\n",
    "print(\"\\nEMA 2021 close:\")\n",
    "print(ema_2021_close[-1])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSI 2021 close:\n",
      "53.76516415158352\n",
      "\n",
      "EMA 2021 close:\n",
      "470.6900882953319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLrYAbIBzJXi"
   },
   "source": [
    "#### 1.1.2 Implementación de StockMarketEnv\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Ejercicio 2 (2.25 ptos):</strong> Define el entorno <code>StockMarketEnv</code> siguiendo las indicaciones aportadas anteriormente. Además, a parte de las típicas funciones de cualquier entorno (<code>reset</code>, <code>step</code> y <code>render</code>), deben implementarse dos funciones más (<code>save_to_csv_file</code> y <code>_normalize</code>) que se explican a continuación:\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3IO8btTNMg2"
   },
   "source": [
    "##### Crear la función <code>save_to_csv_file</code> en Python\n",
    "Además, implementa una función <code>save_to_csv_file</code> que guarde los datos actuales de una clase en un archivo CSV. La función calculará el beneficio (profit) como la diferencia entre el valor neto actual (net_worth-> balance efectivo + valor acciones en posesión) y el balance inicial (initial_balance), y escribirá una nueva fila con los valores de current_step, balance, shares_held, net_worth y profit en el archivo CSV.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Crear la función de normalización en Python\n",
    "\n",
    "La función `_normalize` se utiliza para ajustar un valor a un rango estándar, comúnmente entre 0 y 1, en relación a un valor mínimo y máximo especificados. Este proceso, conocido como **normalización**, es útil para transformar datos, manteniendo sus proporciones relativas, dentro de un intervalo más manejable. La función toma tres parámetros:\n",
    "\n",
    "- `value`: el valor a normalizar.\n",
    "- `min_val`: el límite inferior del rango de normalización.\n",
    "- `max_val`: el límite superior del rango de normalización.\n",
    "\n",
    "La normalización se realiza mediante la siguiente fórmula:\n",
    "\n",
    "$$\n",
    "\\text{normalized_value} = \\frac{\\text{value} - \\text{min_val}}{\\text{max_val} - \\text{min_val}}\n",
    "$$\n",
    "\n",
    "\n",
    "Este cálculo ajusta `value` al rango definido entre `min_val` y `max_val`. Además, si `max_val` y `min_val` son iguales, la función devuelve `0` para evitar una **división por cero**.\n",
    "\n",
    "##### Beneficios en el Contexto de Aprendizaje por Refuerzo (RL)\n",
    "\n",
    "En un entorno de **aprendizaje por refuerzo (RL)**, la normalización es fundamental por varias razones:\n",
    "\n",
    "1. **Estabilidad de Entrenamiento**: Al normalizar las recompensas, las observaciones o las acciones a un rango estándar, se evita que valores grandes desestabilicen el proceso de aprendizaje. Modelos de RL, como las redes neuronales, tienden a aprender mejor con datos en intervalos limitados.\n",
    "\n",
    "2. **Facilita la Comparación**: Normalizar permite comparar datos provenientes de distintas fuentes o escalas, como recompensas de diferentes entornos, lo cual mejora la generalización del modelo.\n",
    "\n",
    "3. **Acelera la Convergencia**: Datos escalados de manera uniforme ayudan a que los algoritmos de RL converjan más rápidamente, ya que se reduce la variabilidad de las entradas.\n",
    "\n",
    "4. **Previene Errores de Cálculo**: Al manejar entradas normalizadas y con límites definidos, se evitan errores de cálculo o inestabilidades debidas a diferencias numéricas extremas.\n",
    "\n",
    "------\n",
    "<b>Nota</b>: se os proporciona el código pre-implementado. La implementación que se pide en el enunciado está indicada en los bloques <i>#TODO</i> y/o con variables igualadas a <i>None</i>."
   ],
   "metadata": {
    "id": "XasqzSbkF6hd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fe9EFgu9zJXi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "start = #TODO\n",
    "end = #TODO\n",
    "ticker = #TODO\n",
    "initial_balance = #TODO\n",
    "\n",
    "class StockMarketEnv(gym.Env):\n",
    "    def __init__(self, ticker=ticker, initial_balance=initial_balance,  is_eval = False,\n",
    "                 start = start, end = end, save_to_csv=False,\n",
    "                 csv_filename=\"stock_trading_log.csv\"):\n",
    "        super(StockMarketEnv, self).__init__()\n",
    "\n",
    "        # Descargar los datos históricos de la acción\n",
    "        self.df = #TODO\n",
    "        self.num_trading_days = len(self.df)\n",
    "        self.prices = #TODO\n",
    "        self.n_steps = len(self.prices)-1\n",
    "\n",
    "        # Parámetros del entorno\n",
    "        self.initial_balance = #TODO\n",
    "        self.current_step = #TODO\n",
    "        self.balance = #TODO\n",
    "        self.shares_held = #TODO\n",
    "        self.net_worth = #TODO\n",
    "        self.previus_net_worth = #TODO\n",
    "\n",
    "        # Espacio de acciones: 0 -> mantener, 1 -> comprar, 2 -> vender\n",
    "        self.action_space = #TODO\n",
    "\n",
    "        # Calculamos los indicadores técnicos\n",
    "        self.rsi = #TODO\n",
    "        self.ema = #TODO\n",
    "\n",
    "\n",
    "        # Espacio de observaciones: [precio_actual, balance, acciones, rsi, ema, sma, upper_band, lower_band]\n",
    "        self.observation_space = #TODO\n",
    "        self.is_eval = is_eval\n",
    "\n",
    "        # Valores para normalización (obtenemos mínimos y máximos)\n",
    "        self.min_price = #TODO\n",
    "        self.max_price = #TODO\n",
    "        self.min_rsi = #TODO\n",
    "        self.max_rsi = #TODO\n",
    "        self.min_ema = #TODO\n",
    "        self.max_ema = #TODO\n",
    "\n",
    "\n",
    "        # Parámetros adicionales para el CSV\n",
    "        self.save_to_csv = #TODO\n",
    "        self.csv_filename = #TODO\n",
    "\n",
    "        # Si la opción de almacenar en CSV está activada, crea o sobreescribe el archivo\n",
    "        if self.save_to_csv:\n",
    "            pass\n",
    "            #TODO\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        #TODO\n",
    "        return self._next_observation(),{}\n",
    "\n",
    "    def _normalize(self, value, min_val, max_val):\n",
    "        #TODO\n",
    "        return normalized_value\n",
    "\n",
    "\n",
    "    def _next_observation(self):\n",
    "        # Normalizamos los valores\n",
    "        norm_price = self._normalize()#TODO\n",
    "        norm_balance = self._normalize(self.balance, self.initial_balance * 0.85, self.initial_balance * 1.25)\n",
    "        norm_shares_held = self._normalize(self.shares_held, 0, 100)  # Máximo de 100 acciones\n",
    "        norm_rsi = self._normalize()#TODO\n",
    "        norm_ema = self._normalize()#TODO\n",
    "\n",
    "        return np.array([\n",
    "            norm_price,\n",
    "            norm_balance,\n",
    "            norm_shares_held,\n",
    "            norm_rsi,\n",
    "            norm_ema,\n",
    "        ])\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = #TODO\n",
    "        reward = #TODO\n",
    "\n",
    "        # Acción: 0 -> mantener, 1 -> comprar, 2 -> vender\n",
    "        #TODO\n",
    "\n",
    "\n",
    "        # Actualizar el precio anterior\n",
    "        self.previus_net_worth = #TODO\n",
    "\n",
    "\n",
    "        # Avanzar al siguiente paso\n",
    "        self.current_step += 1\n",
    "        terminated = #TODO\n",
    "        truncated = #TODO\n",
    "\n",
    "        if self.save_to_csv:\n",
    "            self.save_to_csv_file()\n",
    "\n",
    "\n",
    "\n",
    "        # Devuelve la observación, la recompensa, si está hecho, y otra info adicional\n",
    "        return {} #TODO\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "      #TODO print\n",
    "      #Step: 4\n",
    "      #Balance: 10406.799926757812\n",
    "      #Shares held: 0\n",
    "      #Net worth: 10406.799926757812\n",
    "      #Profit: 406.7999267578125\n",
    "      pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # La función save_to_csv_file guarda los datos actuales en un archivo CSV.\n",
    "    # 1. Primero calcula el beneficio como la diferencia entre el valor neto\n",
    "    # actual y el balance inicial.\n",
    "    # 2. Luego, abre (o crea) el archivo CSV en modo 'append' para agregar una\n",
    "    # nueva fila de datos sin sobrescribir las anteriores.\n",
    "    # 3. Escribe una nueva fila en el CSV con los valores del paso actual,\n",
    "    # balance, acciones mantenidas, valor neto y el beneficio.\n",
    "    # Step,Balance,Shares Held,Net Worth,Profit\n",
    "    # 1,12000,50,13000,3000\n",
    "    def save_to_csv_file(self):\n",
    "        \"\"\"Guarda los datos actuales en el archivo CSV.\"\"\"\n",
    "        #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlH3wQoIPqYv"
   },
   "source": [
    "La siguiente celda es de **comprobación** y debe generar la salida que se muestra a continuación. Esta salida sirve para verificar que todo se ha implementado correctamente. Al ejecutar la celda, la salida debe coincidir exactamente con el siguiente resultado:\n",
    "\n",
    "\n",
    "```\n",
    "------------------------------------------------------------------------------------\n",
    "(array([0.14086006, 0.375     , 0.        , 0.45140651, 0.        ]), 0, False, False, {})\n",
    "------------------------------------------------------------------------------------\n",
    "(array([ 0.19505732, -2.06710007,  0.4       ,  0.45140651,  0.00333123]), 0, False, False, {})\n",
    "------------------------------------------------------------------------------------\n",
    "(array([ 0.20824227, -2.06710007,  0.4       ,  0.45140651,  0.00842359]), 1, False, False, {})\n",
    "------------------------------------------------------------------------------------\n",
    "(array([0.22407732, 0.47669998, 0.        , 0.45140651, 0.01548553]), 1, False, False, {})\n",
    "------------------------------------------------------------------------------------\n",
    "Step: 4  \n",
    "Balance: 10406.799926757812  \n",
    "Shares held: 0  \n",
    "Net worth: 10406.799926757812  \n",
    "Profit: 406.7999267578125  \n",
    "None  \n",
    "------------------------------------------------------------------------------------\n",
    "(array([0.23202811, 0.47669998, 0.        , 0.45140651, 0.02293572]), -1, False, False, {})\n",
    "------------------------------------------------------------------------------------\n",
    "Step: 5  \n",
    "Balance: 10406.799926757812  \n",
    "Shares held: 0  \n",
    "Net worth: 10406.799926757812  \n",
    "Profit: 406.7999267578125  \n",
    "None  \n",
    "------------------------------------------------------------------------------------\n",
    "(array([ 0.23805742, -2.10300003,  0.4       ,  0.45140651,  0.03040101]), 0, False, False, {})\n",
    "------------------------------------------------------------------------------------\n",
    "Step: 6  \n",
    "Balance: 87.9998779296875  \n",
    "Shares held: 40  \n",
    "Net worth: 10406.799926757812  \n",
    "Profit: 406.7999267578125  \n",
    "None  \n",
    "------------------------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_c2GrpVHNMg3"
   },
   "outputs": [],
   "source": [
    "env = StockMarketEnv()\n",
    "env.reset()\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.step(0))\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.step(1))\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.step(1))\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.step(2))\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.render())\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.step(0))\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.render())\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.step(1))\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.render())\n",
    "print('------------------------------------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmlDKL_WzJXi"
   },
   "source": [
    " #### 1.1.3 Interacción con el Entorno StockMarketEnv\n",
    "\n",
    " <div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Ejercicio 3 (0.5 ptos):</strong> Cargar el entorno <code>StockMarketEnv</code> y realizar las siguientes tareas:\n",
    "     <ul> <li>Mostrar el espacio de acciones y el espacio de observaciones.</li>\n",
    "     <li>Ejecutar 100 episodios con acciones aleatorias, mostrando la recompensa media obtenida entre todas las partidas.</li>\n",
    "      <li> Ejecuta una partida aleatoria y mostrar el render al finalizar la misma.</li>\n",
    "      <li>Probar la función <code>save_to_csv_file</code> y mostrar el resultado de las últimas 5 filas.</li>\n",
    "     </ul>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pt0BhGGJzJXi",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = StockMarketEnv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tldZjjBlzJXi"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxM9K02yNMg3"
   },
   "outputs": [],
   "source": [
    "episodes = 100\n",
    "mean_reward = 0\n",
    "#TODO\n",
    "\n",
    "print(f'\\nMean reward over {episodes} episodes: {mean_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EK54MX40NMg3"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "#TODO\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqRtVBGI1RVs"
   },
   "outputs": [],
   "source": [
    "def probar_save_to_csv(env, rows ):\n",
    "    \"\"\"\n",
    "    Probar la función save_to_csv_file ejecutando varias acciones\n",
    "    en el entorno y luego mostrando las últimas 5 filas del archivo CSV.\n",
    "\n",
    "    Parámetros:\n",
    "        env: El entorno StockMarketEnv.\n",
    "        num_steps: El número de pasos que se desean ejecutar.\n",
    "    \"\"\"\n",
    "    # Ejecutar pasos en el entorno con acciones aleatorias\n",
    "    #TODO\n",
    "\n",
    "\n",
    "    # Leer y mostrar las últimas x filas del archivo CSV\n",
    "    if env.save_to_csv:\n",
    "        # Leer el archivo CSV en un DataFrame\n",
    "        #TODO\n",
    "        pass\n",
    "\n",
    "env = StockMarketEnv(ticker=\"SPY\", start=\"2019-01-01\", end=\"2021-01-01\", save_to_csv=True, csv_filename=\"stock_trading_log.csv\")\n",
    "probar_save_to_csv(env, 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_ZXX9atzJXj"
   },
   "source": [
    "## 2. Agente DQN inversor en bolsa (2 ptos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CgCRw341JS7"
   },
   "source": [
    "En este apartado implementaremos una DQN teniendo en cuenta la exploración-explotación (epsilon-*greedy*), la red objetivo, y el buffer de repetición de experiencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90l7Ba721JS7"
   },
   "source": [
    "Definiremos el buffer como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPdwD1G81JS7"
   },
   "outputs": [],
   "source": [
    "class experienceReplayBuffer:\n",
    "\n",
    "    def __init__(self, memory_size=50000, burn_in=10000):\n",
    "        self.memory_size = memory_size\n",
    "        self.burn_in = burn_in\n",
    "        self.buffer = namedtuple('Buffer',\n",
    "            field_names=['state', 'action', 'reward', 'done', 'next_state'])\n",
    "        self.replay_memory = deque(maxlen=memory_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        samples = np.random.choice(len(self.replay_memory), batch_size,\n",
    "                                   replace=False)\n",
    "        # Use asterisk operator to unpack deque\n",
    "        batch = zip(*[self.replay_memory[i] for i in samples])\n",
    "        return batch\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.replay_memory.append(\n",
    "            self.buffer(state, action, reward, done, next_state))\n",
    "\n",
    "    def burn_in_capacity(self):\n",
    "        return len(self.replay_memory) / self.burn_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvY3iwTP1JS7"
   },
   "source": [
    "### 2.1 Implementación de la Clase NeuralNetStockMarket\n",
    "\n",
    "<br>Primeramente implementaremos la red neuronal, utilizando un modelo Secuencial con la siguiente configuración:</br>\n",
    "<ul>\n",
    "<li>\n",
    "Tres capas completamente conectadas (representadas en pytorch por nn.Lineal) con 256, 128 y 64 neuronas cada una, bias=True, y activación ReLU</li>\n",
    "<li>Una capa de salida completamente conectada y bias=True</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISy3TJ5s1JS7"
   },
   "source": [
    "Usaremos el optimizador Adam para entrenar la red.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-FNzJNO1JS8"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Ejercicio (0.5 ptos):</strong> Implementar la clase <code>NeuralNetStockMarket:()</code>. Inicializar las variables necesarias y definir el modelo Secuencial de red neuronal indicado.\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se os proporciona el código pre-implementado. La implementación que se pide en el enunciado está indicada en los bloques <i>#TODO</i> y/o con variables igualadas a <i>None</i>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ql0g4IKE1JS8"
   },
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetStockMarket(torch.nn.Module):\n",
    "\n",
    "    ###################################\n",
    "    ###inicialización y modelo###\n",
    "    def __init__(self, env, learning_rate=1e-3, optimizer = None, device=None):\n",
    "\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        n_inputs: tamaño del espacio de estados\n",
    "        n_outputs: tamaño del espacio de acciones\n",
    "        actions: array de acciones posibles\n",
    "        \"\"\"\n",
    "        ######################################\n",
    "        ##TODO: Inicializar parámetros\n",
    "        super(NeuralNetStockMarket, self).__init__()\n",
    "        self.n_inputs = None\n",
    "        self.n_outputs = None\n",
    "        self.actions = None\n",
    "        self.learning_rate = None\n",
    "\n",
    "        # Definir el dispositivo (CPU o GPU)\n",
    "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "        #######################################\n",
    "        ##Construcción de la red neuronal\n",
    "        self.model = None #TODO\n",
    "\n",
    "        #######################################\n",
    "        ##Inicializar el optimizador\n",
    "        if optimizer is None:\n",
    "            self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "\n",
    "\n",
    "\n",
    "    ### MéTODO e-greedy\n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = None #TODO  # acción aleatoria\n",
    "        else:\n",
    "            qvals = None  #TODO  # acción a partir del cálculo del valor de Q para esa acción\n",
    "            action= torch.max(qvals, dim=-1)[1].item()\n",
    "        return action\n",
    "\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        if type(state) is tuple:\n",
    "            state = np.array([np.ravel(s) for s in state])\n",
    "        state_t = torch.FloatTensor(state).to(self.device)\n",
    "        return self.model(state_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEfgOTll1JS8"
   },
   "source": [
    "### 2.2 Implementación del Agente DQN con Exploración/Explotación y Sincronización de Redes\n",
    "\n",
    "A continuación implementaremos una clase que defina el comportamiento del agente DQN teniendo en cuenta:\n",
    "\n",
    "La exploración/explotación (decaimiento de epsilon)\n",
    "La actualización y sincronización de la red principal y la red objetivo (pérdida)\n",
    "Consideraremos que el agente ha aprendido a realizar la tarea (i.e. el \"juego\" termina) cuando obtiene una media de mínimo 8700$ durante 100 episodios consecutivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQVGVM6T1JS8"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.5 ptos):</strong> Implementar los siguientes puntos de la clase <code>DQNAgent()</code>:\n",
    "    <ol>\n",
    "        <li>Declarar las variables de la clase</li>\n",
    "        <li>Inicializar las variables necesarias</li>\n",
    "        <li>Implementar la acción a tomar</li>\n",
    "        <li>Actualizar la red principal según la frecuencia establecida en los hiperparámetros</li>\n",
    "        <li>Calcular la pérdida (ecuación Bellman, etc)</li>\n",
    "        <li>Sincronizar la red objetivo según la frecuencia establecida en los hiperparámetros</li>\n",
    "        <li>Calcular la media de recompensas de los últimos 100 episodios</li>\n",
    "         <li>Comprobar límite de episodios</li>\n",
    "        <li>Actualizar epsilon según: $$ \\textrm{max}(\\epsilon · \\epsilon_{\\textrm{decay}}, 0.01) $$ </li>\n",
    "    </ol>\n",
    "Además, durante el proceso se deben almacenar (*):\n",
    "    <ol>\n",
    "        <li>Las recompensas obtenidas en cada paso del entrenamiento</li>\n",
    "        <li>Las recompensas medias cada 100 episodios</li>\n",
    "        <li>La pérdida durante el entrenamiento</li>\n",
    "        <li>La evolución de epsilon a lo largo del entrenamiento</li>\n",
    "                 <li>Almacena la cantidad de episodios necesarios para llevar acabo el entrenamiento en la variable episodes_train_dqn </li>\n",
    "    </ol>\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se os proporciona el código pre-implementado. La implementación que se pide en el enunciado está indicada en los bloques <i>#TODO</i> y/o con variables igualadas a <i>None</i>, salvo (*) en qué momento almacenar las variables que se indican.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQcTPvAE1JS8"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env, main_network, buffer, epsilon=0.1, eps_decay=0.99, batch_size=32, min_episodes= 300, device = None):\n",
    "        ######################################\n",
    "        ##TODO 1: Declarar variables\n",
    "        self.env = None\n",
    "        self.main_network = None\n",
    "        self.target_network = None # red objetivo (copia de la principal)\n",
    "        self.buffer = None\n",
    "        self.epsilon = None\n",
    "        self.eps_decay = None\n",
    "        self.batch_size = None\n",
    "        self.nblock = None # bloque de los X últimos episodios de los que se calculará la media de recompensa\n",
    "        self.initialize()\n",
    "        self.min_episodes = None\n",
    "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Configurar el dispositivo (CPU o GPU)\n",
    "\n",
    "\n",
    "    def initialize(self):\n",
    "        ######################################\n",
    "        ##TODO 3: Inicializa lo necesario\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## Tomar nueva acción\n",
    "    def take_step(self, eps, mode='train'):\n",
    "        if mode == 'explore':\n",
    "            # acción aleatoria en el burn-in y en la fase de exploración (epsilon)>\n",
    "            action = None\n",
    "        else:\n",
    "            # acción a partir del valor de Q (elección de la acción con mejor Q)\n",
    "            action = None\n",
    "            self.step_count += 1\n",
    "        #TODO: tomar 'step' i obtener nuevo estado y recompensa. Guardar la experiencia en el buffer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #TODO: resetear entorno 'if done'\n",
    "\n",
    "\n",
    "\n",
    "    ## Entrenamiento\n",
    "    def train(self, gamma=0.99, max_episodes=50000,\n",
    "              batch_size=32,\n",
    "              dnn_update_frequency=4,\n",
    "              dnn_sync_frequency=2000, REWARD_THRESHOLD = 9000):\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Rellenamos el buffer con N experiencias aleatorias ()\n",
    "        print(\"Filling replay buffer...\")\n",
    "        while self.buffer.burn_in_capacity() < 1:\n",
    "            self.take_step(self.epsilon, mode='explore')\n",
    "\n",
    "\n",
    "        episode = 0\n",
    "        training = True\n",
    "        print(\"Training...\")\n",
    "        while training:\n",
    "            self.state0 = self.env.reset()[0]\n",
    "            self.total_reward = 0\n",
    "            gamedone = False\n",
    "            while gamedone == False:\n",
    "                # El agente toma una acción\n",
    "                gamedone = self.take_step(self.epsilon, mode='train')\n",
    "                ##################################################################################\n",
    "                #####TODO 4:  Actualizar la red principal según la frecuencia establecida #######\n",
    "\n",
    "\n",
    "\n",
    "                ########################################################################################\n",
    "                ###TODO 6: Sincronizar red principal y red objetivo según la frecuencia establecida#####\n",
    "\n",
    "\n",
    "\n",
    "                if gamedone:\n",
    "                    episode += 1\n",
    "                    #######################################################################################\n",
    "                    ###TODO 7: calcular la media de recompensa de los últimos X episodios, y almacenar#####\n",
    "\n",
    "                    self.update_loss = []\n",
    "\n",
    "\n",
    "\n",
    "                    #######################################################################################\n",
    "                    ### TODO 8: Comprobar que todavía quedan episodios. Parar el aprendizaje si se llega al límite\n",
    "\n",
    "\n",
    "                    print(\"\\rEpisode {:d} Mean Rewards {:.2f} Epsilon {}\\t\\t\".format(\n",
    "                        episode, mean_rewards, self.epsilon), end=\"\")\n",
    "\n",
    "                    # Comprobamos que todavía quedan episodios\n",
    "                    if episode >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                            episode))\n",
    "                        break\n",
    "\n",
    "                    #######################################################################################\n",
    "                    ### TODO 9: Añadir min episodes\n",
    "                    if mean_rewards >= REWARD_THRESHOLD and self.min_episodes < episode :\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                            episode))\n",
    "                        break\n",
    "\n",
    "\n",
    "                    #################################################################################\n",
    "                    ######TODO 9: Actualizar epsilon según la velocidad de decaimiento fijada########\n",
    "\n",
    "\n",
    "    ## Cálculo de la pérdida\n",
    "    def calculate_loss(self, batch):\n",
    "      #  print('loss')\n",
    "        # Separamos las variables de la experiencia y las convertimos a tensores\n",
    "        states, actions, rewards, dones, next_states = [i for i in batch]\n",
    "        rewards_vals = torch.FloatTensor(rewards).to(self.device)\n",
    "        actions_vals = torch.LongTensor(np.array(actions)).reshape(-1,1).to(self.device)\n",
    "        dones_t = torch.tensor(dones, dtype=torch.bool).to(self.device)\n",
    "\n",
    "        # Obtenemos los valores de Q de la red principal\n",
    "        qvals = torch.gather(self.main_network.get_qvals(states), 1, actions_vals).to(self.device)\n",
    "        # Obtenemos los valores de Q objetivo. El parámetro detach() evita que estos valores actualicen la red objetivo\n",
    "        qvals_next = torch.max(self.target_network.get_qvals(next_states),\n",
    "                               dim=-1)[0].detach().to(self.device)\n",
    "        qvals_next[dones_t.bool()] = 0\n",
    "\n",
    "        #################################################################################\n",
    "        ### TODO: Calcular ecuación de Bellman\n",
    "        expected_qvals = None\n",
    "\n",
    "        #################################################################################\n",
    "        ### TODO: Calcular la pérdida (MSE)\n",
    "        loss = None\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        self.main_network.optimizer.zero_grad()  # eliminamos cualquier gradiente pasado\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size) # seleccionamos un conjunto del buffer\n",
    "        loss = self.calculate_loss(batch) # calculamos la pérdida\n",
    "        loss.backward() # hacemos la diferencia para obtener los gradientes\n",
    "        self.main_network.optimizer.step() # aplicamos los gradientes a la red neuronal\n",
    "        # Guardamos los valores de pérdida\n",
    "        self.update_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70dAg7v71JS9"
   },
   "source": [
    "### 2.3 Entrenamiento del Modelo\n",
    "\n",
    "\n",
    "(0.5 ptos) A continuación entrenaremos el modelo con los siguientes hiperparámetros:\n",
    "   <ul>\n",
    "        <li>Velocidad de aprendizaje: 0.0005</li>\n",
    "        <li>Tamaño del batch: 128</li>\n",
    "        <li>Número de episodios: 4000</li>\n",
    "        <li>Número de episodios para rellenar el buffer (BURN_IN): 1000</li>\n",
    "        <li>Frecuencia de actualización de la red neuronal: 6 </li>\n",
    "        <li>Frecuencia de sincronización con la red objetivo: 15</li>\n",
    "        <li>Capacidad máxima del buffer (MEMORY_SIZE ): 50000</li>\n",
    "        <li>Factor de descuento: 0.99</li>\n",
    "        <li>Epsilon: 1, con decaimiento de 0.995</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBFZNwKA1JS9"
   },
   "outputs": [],
   "source": [
    "#TODO deficinición de variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7kU6ANy1NMg8"
   },
   "outputs": [],
   "source": [
    "\n",
    "#TODO calcular el REWARD_THRESHOLD\n",
    "ticker = 'SPY'\n",
    "start = ''\n",
    "end = ''\n",
    "num_days = 0\n",
    "print(f\"Numero de dias de tradring para {ticker} desde {start} hasta {end}: {num_days}\")\n",
    "print(f\"Nuestro objetivo ganar el 50 por ciento de los dias: {round(num_days/2)}\")\n",
    "REWARD_THRESHOLD = round(num_days/2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBl792X11JS9"
   },
   "outputs": [],
   "source": [
    "#TODO entrenamiento..\n",
    "#Training time en Google Colab en GPU: 42.53 minutes\n",
    "#De media obtiene enre 170-190 de puntuación y alcanza los 4000 episodios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff6qg_fV1JS9"
   },
   "source": [
    "### 2.4 Análisis del entrenamiento\n",
    "\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.25 ptos):</strong> Representar:\n",
    "    <ol>\n",
    "        <li>Gráfico con las recompensas obtenidas a lo largo del entrenamieno, la evolución de las recompensas medias cada 100 episodios, y el umbral de recompensa establecido por el entorno.</li>\n",
    "        <li>Gráfico con la evolución de la perdida a lo largo del entrenamiento</li>\n",
    "        <li>Gráfico con la evolución de epsilon a lo largo del entrenamiento</li>\n",
    "    </ol>\n",
    "\n",
    "Comentar los resultados obtenidos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6tyd9Z11JS-"
   },
   "outputs": [],
   "source": [
    "def plot_rewards(tr_rewards, mean_tr_rewards, th):\n",
    "    #TODO\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwBsmpUz1JS-"
   },
   "outputs": [],
   "source": [
    "def plot_loss(tr_loss):\n",
    "    #TODO\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1os-TdE1JS-"
   },
   "outputs": [],
   "source": [
    "def plot_epsilon(eps_evolution):\n",
    "    #TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ccu9YyPX1JS-"
   },
   "outputs": [],
   "source": [
    "#TODO mostrar gráficas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRc9q-LB1JS-"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentarios:</strong>\n",
    "#TODO\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxuZP8vS1JS-"
   },
   "source": [
    "Una vez entrenado el agente, nos interesa comprobar cómo de bien ha aprendido y si es capaz de conseguir superar el entorno. Para ello, recuperamos el modelo entrenado y dejamos que el agente tome acciones aleatorias según ese modelo y observamos su comportamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcbEPSqp1JS-"
   },
   "source": [
    "### 2.4 Test del agente.\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.25 ptos):</strong> Cargar el modelo entrenado y ejecutar el agente entrenado durante 505 episodios consecutivos en diferentes periodos aleatorios desde el año 2015 hasta el 2024. Calcula la suma de recompensas por cada ejecución. Para conseguir este punto, ejecuta:\n",
    "    <ul>\n",
    "        <li>Un gráfico con la suma de las recompensas respecto de los episodios, incluyendo el umbral de recompensa establecido</li>\n",
    "        <li>Almacena la recompensa media obtenida en las 100 partidas en la variable <code>mean_reward_dqn_test</code> y la última recompensa obtenida en el entrenamiento en <code>mean_reward_dqn_last</code>. También obten en cuantos escenarios se ha obtenido más de 252 dias positivos en el trading. </li>\n",
    "    </ul>\n",
    "Además, realiza la siguiente análisis con el modelo para el entorno utilizado en el entrenamiento durante :\n",
    " <ul>\n",
    " <li>Reproducir una partida completa del agente entrenado y mostrar el resultado final, incluyendo el valor total del portafolio al final del episodio.</li>\n",
    "        <li>Generar un fichero CSV que registre los resultados de las interacciones del agente con el mercado en cada episodio y muestra por pantalla las últimas 30 acciones.</li>\n",
    " </ul>\n",
    "<strong>Comenta todos los resultados obtenidos en este apartado. ¿A qué conclusiones podemos llegar? ¿Cómo podríamos mejorar el entrenamiento y qué implicaciones tendría?</strong>\n",
    "\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijUTlkn91JS_"
   },
   "outputs": [],
   "source": [
    "#Generar un fichero CSV que registre los resultados de las interacciones\n",
    "#del agente con el mercado en cada episodio y muestra por pantalla las últimas 30 acciones.\n",
    "file_path = 'stock_trading_agent_dqn.csv'\n",
    "#Reproducir una partida completa del agente entrenado y mostrar el resultado final,\n",
    "#incluyendo el valor total del portafolio al final del episodio.\n",
    "env = ''#TODO\n",
    "\n",
    "\n",
    "def read_csv_and_show_last_30(file_path):\n",
    "    try:\n",
    "        #TODO\n",
    "        pass\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"El archivo {file_path} no fue encontrado.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Se produjo un error al leer el archivo: {e}\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "read_csv_and_show_last_30(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Df3VnU3i1JS_"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generar calendario de trading usando días hábiles\n",
    "def generate_random_trading_dates(start_range, end_range, trading_days_target=505):\n",
    "    \"\"\"\n",
    "    Genera un par de fechas (start, end) que tengan exactamente trading_days_target días hábiles.\n",
    "    \"\"\"\n",
    "    start_date = pd.to_datetime(start_range)\n",
    "    end_date = pd.to_datetime(end_range)\n",
    "\n",
    "    while True:\n",
    "        # Seleccionar una fecha de inicio aleatoria\n",
    "        random_start = start_date + pd.DateOffset(days=random.randint(0, (end_date - start_date).days - trading_days_target))\n",
    "\n",
    "        # Generar un rango de fechas de trading usando solo los días hábiles\n",
    "        trading_days = pd.bdate_range(random_start, random_start + pd.DateOffset(days=2 * trading_days_target)).tolist()\n",
    "\n",
    "        # Filtrar las fechas para obtener exactamente el número de días objetivo\n",
    "        if len(trading_days) >= trading_days_target:\n",
    "            random_end = trading_days[trading_days_target - 1]  # Último día de trading en el rango deseado\n",
    "            return random_start.strftime(\"%Y-%m-%d\"), random_end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def test_model(ag, base_env, start_range, end_range, trading_days_target=505, win_days_target=252):\n",
    "    all_rewards = []\n",
    "    win_days_count = []\n",
    "\n",
    "    for i_episode in range(100):\n",
    "        # Generar nuevas fechas de inicio y fin aleatorias que cumplan con los días de trading deseados\n",
    "        start_date, end_date = generate_random_trading_dates(start_range, end_range, trading_days_target)\n",
    "\n",
    "        # Actualizar el entorno con las nuevas fechas\n",
    "        env = None  #TODO\n",
    "\n",
    "        #TODO\n",
    "\n",
    "        env.close()\n",
    "\n",
    "\n",
    "\n",
    "    return all_rewards,success_rate\n",
    "\n",
    "\n",
    "def plot_test(rewards, th):\n",
    "    pass\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nf-dQTJSNMg-"
   },
   "outputs": [],
   "source": [
    "mean_reward_dqn = 0 #TODO media de las 100 partidas de test\n",
    "mean_reward_dqn_last = 0 #TODO Mean Reward la última iteración del entrenamiento.\n",
    "success_rate = 0\n",
    "print(f\"La recompensa media obtenida por el agente DQN en las 100 partidas de test es: {mean_reward_dqn:.2f} puntos.\")\n",
    "print(f\"Porcentaje de episodios que lograron ganar al menos 252 días: {success_rate * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zivDev7J1JS_"
   },
   "source": [
    "\n",
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentarios:</strong>\n",
    "#TODO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLVL98bV1JS_"
   },
   "source": [
    "## 3. Agente Dueling DQN (1.5 ptos)\n",
    "\n",
    "En este apartado resolveremos el mismo entorno con las mismas características para el agente, pero usando una dueling DQN. Como en el caso anterior, primero definiremos el modelo de red neuronal, luego describiremos el comportamiento del agente, lo entrenaremos y, finalmente, testearemos el funcionamiento del agente entrenado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puwBgnP41JS_"
   },
   "source": [
    "### 3.1 Definición de la arquitectura  de la red neuronal\n",
    "\n",
    "\n",
    "El objetivo principal de las dueling DQN es \"ahorrarse\" el cálculo del valor de Q en aquéllos estados en los que es irrelevante la acción que se tome. Para ello se descompone la función Q en dos componentes:\n",
    "\n",
    "\n",
    "$$Q(s, a) = A(s, a) + V (s)$$\n",
    "\n",
    "\n",
    "Esta descomposición se realiza a nivel de la arquitectura de la red neuronal. Las primeras capas que teníamos en la DQN serán comunes, y luego la red se dividirá en dos partes separadas definidas por el resto de capas.\n",
    "\n",
    "\n",
    "La descomposición en sub-redes del modelo de la DQN implementada en el apartado anterior, será entonces:\n",
    "\n",
    "<ol> <li> Bloque común: </li> <ul> <li>Una primera capa completamente conectada de 256 neuronas y <code>bias = True</code>, con activación ReLU </li\n",
    "<li>Una primera capa completamente conectada de 128 neuronas y <code>bias = True</code>, con activación ReLU </li\n",
    "\n",
    "> </ul> <li>Para cada una de las subredes de ventaja A(s,a) y valor V(s):</li> <ul> <li>Una capa completamente conectada de 64 neuronas y <code>bias = True</code>, con activación ReLU </li> <li>Una última capa completamente conectada y <code>bias = True</code>. Esta será nuestra capa de salida y por tanto el número de neuronas de salida dependerá de si se trata de la red A(s,a), que tendrá tantas neuronas como dimensiones tenga el espacio de acciones, o si se trata de la red V(s), con un valor por estado.</li> </ul> </ol>\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGxOz5G21JS_"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 3.1 (0.5 ptos):</strong> Implementar la clase <code>duelingDQN()</code>. Inicializar las variables necesarias y definir el modelo de red neuronal indicado.\n",
    "\n",
    "    -----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se os proporciona el código pre-implementado. La implementación que se pide en el enunciado está indicada en los bloques <i>#TODO</i> y/o con variables igualadas a <i>None</i>.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltuTJg4w1JTA"
   },
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "\n",
    "class duelingDQN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, env, device=None, learning_rate=1e-3):\n",
    "\n",
    "        \"\"\"\n",
    "        Parámetros\n",
    "        ==========\n",
    "        n_inputs: tamaño del espacio de estados\n",
    "        n_outputs: tamaño del espacio de acciones\n",
    "        actions: array de acciones posibles\n",
    "        \"\"\"\n",
    "\n",
    "        ###################################\n",
    "        ####TODO: Inicializar variables####\n",
    "        super(duelingDQN, self).__init__()\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.n_inputs = #TODO\n",
    "        self.n_outputs = #TODO\n",
    "        self.actions = #TODO\n",
    "\n",
    "        ######\n",
    "\n",
    "        #######################################\n",
    "        ##TODO: Construcción de la red neuronal\n",
    "        # Red común\n",
    "        ##Construcción de la red neuronal\n",
    "\n",
    "        self.model_common = #TODO\n",
    "\n",
    "        # Subred de la función de Valor\n",
    "        self.fc_layer_inputs = self.feature_size()\n",
    "\n",
    "\n",
    "        self.advantage  = #TODO\n",
    "\n",
    "        # Recordad adaptarlas a cpu o gpu\n",
    "\n",
    "        # Subred de la Ventaja A(s,a)\n",
    "        self.value = #TODO\n",
    "\n",
    "        #######\n",
    "        #######################################\n",
    "        ##TODO: Inicializar el optimizador\n",
    "        self.optimizer = #TODO\n",
    "\n",
    "\n",
    "    #######################################\n",
    "    #####TODO: función forward#############\n",
    "    def forward(self, state):\n",
    "        # Conexión entre capas de la red común\n",
    "        common_out = #TODO\n",
    "\n",
    "        # Conexión entre capas de la Subred de Valor\n",
    "        advantage = #TODO\n",
    "\n",
    "        # Conexión entre capas de la Subred de Ventaja\n",
    "        value = #TODO\n",
    "\n",
    "\n",
    "        ## Agregar las dos subredes:\n",
    "        # Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\n",
    "        action = #TODO\n",
    "\n",
    "        return action\n",
    "    #######\n",
    "\n",
    "\n",
    "\n",
    "    ### MéTODO e-greedy\n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            qvals = self.get_qvals(state)\n",
    "            action = torch.max(qvals, dim=-1)[1].item()\n",
    "        return action\n",
    "\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        if type(state) is tuple:\n",
    "            state = np.array([np.ravel(s) for s in state])\n",
    "        state_t = torch.FloatTensor(state).to(self.device)\n",
    "        return self.forward(state_t)\n",
    "\n",
    "    def feature_size(self):\n",
    "        dummy_input = torch.zeros(1, *env.observation_space.shape).to(self.device)\n",
    "        return self.model_common(autograd.Variable(dummy_input)).view(1, -1).size(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-QAliFGBBy7"
   },
   "source": [
    "Para el buffer de repetición de experiencias podemos usar exactamente la misma clase experienceReplayBuffer descrita en el apartado anterior de la DQN.\n",
    "\n",
    "\n",
    "### 3.2 Definición del agente\n",
    "\n",
    "La diferencia entre la DQN y la dueling DQN se centra, como hemos visto, en la definición de la arquitectura de la red. Pero el proceso de aprendizaje y actualización es exactamente el mismo. Así, podemos recuperar la clase implementada en el apartado anterior, DQNAgent() y reutilizarla aquí bajo el nombre de duelingDQNAgent(). Lo único que deberemos hacer es añadir el optimizador entre las variables a declarar y adaptar la función de pérdida al formato Functional de pytorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXnacQ_e1JTA"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 3.2 (0.25 pto):</strong> Implementar la clase <code>duelingDQNAgent()</code> como la <code>DQNAgent()</code>\n",
    "<p>\n",
    "</p>\n",
    "De nuevo, durante el proceso se deben almacenar (*):\n",
    "    <ul>\n",
    "        <li>Las recompensas obtenidas en cada paso del entrenamiento</li>\n",
    "        <li>Las recompensas medias de los 100 episodios anteriores</li>\n",
    "        <li>La pérdida durante el entrenamiento</li>\n",
    "        <li>La evolución de epsilon a lo largo del entrenamiento</li>\n",
    "    </ul>\n",
    "\n",
    "    -----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se os proporciona el código pre-implementado. La implementación que se pide en el enunciado está indicada en los bloques <i>#TODO</i> y/o con variables igualadas a <i>None</i>, salvo (*) en qué momento almacenar las variables que se indican."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0FuC3kM1JTA"
   },
   "outputs": [],
   "source": [
    "class duelingDQNAgent:\n",
    "\n",
    "    def __init__(self, env, main_network, buffer, reward_threshold, epsilon=0.1, eps_decay=0.99, batch_size=32, device= None):\n",
    "        \"\"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        env: entorno\n",
    "        target_network: clase con la red neuronal diseñada\n",
    "        target_network: red objetivo\n",
    "        buffer: clase con el buffer de repetición de experiencias\n",
    "        epsilon: epsilon\n",
    "        eps_decay: epsilon decay\n",
    "        batch_size: batch size\n",
    "        nblock: bloque de los X últimos episodios de los que se calculará la media de recompensa\n",
    "        reward_threshold: umbral de recompensa definido en el entorno\n",
    "        \"\"\"\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        ###############################################################\n",
    "        #####TODO 1: inicialitzar variables######\n",
    "\n",
    "        self.env = None #TODO\n",
    "        self.main_network =None #TODO\n",
    "        self.target_network =None #TODO  # red objetivo (copia de la principal)\n",
    "        self.buffer =None #TODO\n",
    "        self.epsilon =None #TODO\n",
    "        self.eps_decay =None #TODO\n",
    "        self.batch_size =None #TODO\n",
    "        self.nblock =None #TODO # bloque de los X últimos episodios de los que se calculará la media de recompensa\n",
    "        self.reward_threshold = reward_threshold #umbral de recompensa definido en el entorno\n",
    "\n",
    "        self.initialize()\n",
    "\n",
    "\n",
    "    ###############################################################\n",
    "    #####TODO 2: inicialitzar variables extra que se necessiten######\n",
    "    def initialize(self):\n",
    "        pass\n",
    "        #TODO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #################################################################################\n",
    "    ######TODO 3:  Tomar nueva acción ###############################################\n",
    "    def take_step(self, eps, mode='train'):\n",
    "        if mode == 'explore':\n",
    "            action = None  # TODO acción aleatoria en el burn-in\n",
    "        else:\n",
    "           action = None  # TODO  acción a partir del valor de Q (elección de la acción con mejor Q)\n",
    "            self.step_count += 1\n",
    "\n",
    "        #TODO: Realización de la acción y obtención del nuevo estado y la recompensa\n",
    "\n",
    "        #TODO: resetear entorno 'if done'\n",
    "        if done:\n",
    "            pass #TODO\n",
    "        return done\n",
    "\n",
    "\n",
    "    ## Entrenamiento\n",
    "    def train(self, gamma=0.99, max_episodes=50000,\n",
    "              batch_size=32,\n",
    "              dnn_update_frequency=4,\n",
    "              dnn_sync_frequency=2000, min_episodios=250):\n",
    "        self.gamma = gamma\n",
    "        # Rellenamos el buffer con N experiencias aleatorias ()\n",
    "        print(\"Filling replay buffer...\")\n",
    "        while self.buffer.burn_in_capacity() < 1:\n",
    "            self.take_step(self.epsilon, mode='explore')\n",
    "\n",
    "        episode = 0\n",
    "        training = True\n",
    "        print(\"Training...\")\n",
    "        while training:\n",
    "            self.state0 = self.env.reset()[0]\n",
    "            self.total_reward = 0\n",
    "            gamedone = False\n",
    "            while gamedone == False:\n",
    "                # El agente toma una acción\n",
    "                gamedone = self.take_step(self.epsilon, mode='train')\n",
    "\n",
    "                #################################################################################\n",
    "                #####TODO 4: Actualizar la red principal según la frecuencia establecida  #######\n",
    "\n",
    "                ########################################################################################\n",
    "                ###TODO6: Sincronizar red principal y red objetivo según la frecuencia establecida#####\n",
    "\n",
    "\n",
    "                if gamedone:\n",
    "                    episode += 1\n",
    "                    ##################################################################\n",
    "                    ########TODO: Almacenar epsilon, training rewards i loss#######\n",
    "\n",
    "                    ####\n",
    "                    self.update_loss = []\n",
    "\n",
    "\n",
    "                    #######################################################################################\n",
    "                    ###TODO 7: calcular la media de recompensa de los últimos X episodios, y almacenar#####\n",
    "                    mean_rewards = None\n",
    "                    ###\n",
    "\n",
    "                    print(\"\\rEpisode {:d} Mean Rewards {:.2f} Epsilon {}\\t\\t\".format(\n",
    "                        episode, mean_rewards, self.epsilon), end=\"\")\n",
    "\n",
    "                    # Comprobar si se ha llegado al máximo de episodios\n",
    "                    if episode >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        break\n",
    "\n",
    "                    # Termina el juego si la media de recompensas ha llegado al umbral fijado para este juego\n",
    "                    # y se ha entrenado un mínimo de episodios\n",
    "                    if mean_rewards >= self.reward_threshold and min_episodios <  episode:\n",
    "                        training = False\n",
    "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                            episode))\n",
    "                        break\n",
    "\n",
    "                    #################################################################################\n",
    "                    ######TODO 8: Actualizar epsilon ########\n",
    "                    self.epsilon = None\n",
    "\n",
    "     ## Cálculo de la pérdida\n",
    "    def calculate_loss(self, batch):\n",
    "        # Separamos las variables de la experiencia y las convertimos a tensores\n",
    "        states, actions, rewards, dones, next_states = [i for i in batch]\n",
    "        rewards_vals = torch.FloatTensor(rewards).to(self.device).reshape(-1,1)\n",
    "        actions_vals = torch.LongTensor(np.array(actions)).reshape(-1,1).to(self.device)\n",
    "        dones_t = torch.ByteTensor(dones).to(self.device)\n",
    "\n",
    "        # Obtenemos los valores de Q de la red principal\n",
    "        qvals = torch.gather(self.main_network.get_qvals(states).to(self.device), 1, actions_vals)\n",
    "\n",
    "        #update#\n",
    "        next_actions = torch.max(self.main_network.get_qvals(next_states).to(self.device), dim=-1)[1]\n",
    "        next_actions_vals = next_actions.reshape(-1, 1).to(self.device)\n",
    "\n",
    "\n",
    "        # Obtenemos los valores de Q de la red objetivo\n",
    "        target_qvals = self.target_network.get_qvals(next_states).to(self.device)\n",
    "        qvals_next = torch.gather(target_qvals, 1, next_actions_vals).detach()\n",
    "        #####\n",
    "        qvals_next[dones_t.bool()] = 0\n",
    "        #qvals_next[dones_t] = 0 # 0 en estados terminales\n",
    "        # Calculamos ecuación de Bellman\n",
    "        expected_qvals = None\n",
    "        #Función Loss #####\n",
    "        loss = torch.nn.MSELoss()(qvals, expected_qvals.reshape(-1,1))\n",
    "        #######\n",
    "        return loss\n",
    "\n",
    "    def update(self):\n",
    "        self.main_network.optimizer.zero_grad()  # eliminamos cualquier gradiente pasado\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size) # seleccionamos un conjunto del buffer\n",
    "        loss = self.calculate_loss(batch) # calculamos la pérdida\n",
    "        loss.backward() # hacemos la diferencia para obtener los gradientes\n",
    "        self.main_network.optimizer.step() # aplicamos los gradientes a la red neuronal\n",
    "        # Guardamos los valores de pérdida\n",
    "        if self.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhxbHDHLBYYU"
   },
   "source": [
    "### 3.3 Entrenamiento del Modelo\n",
    "\n",
    "A continuación entrenaremos el modelo dueling DQN con los mismos hiperparámetros con los que entrenamos la DQN.\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 3.3 (0.25 ptos):</strong> Cargar el modelo de red neuronal y entrenar el agente con los mismos hiperparámetros usados para la DQN\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzDK0jVGBnyC"
   },
   "outputs": [],
   "source": [
    "#TODO Tiempo ejecución 69 mintuos en google colaboratory con GPU.\n",
    "#resultado esperado alrededor de 180-200 puntos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlGKaJwyB5ci"
   },
   "source": [
    "### 3.4 Análisis del entrenamiento\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 3.4 (0.25 ptos):</strong> Mostrar los mismos gráficos que con la DQN:\n",
    "    <ol>\n",
    "        <li>Recompensas obtenidas a lo largo del entrenamieno y la evolución de las recompensas medias cada 100 episodios, junto con el umbral de recompensa establecido por el entorno</li>\n",
    "        <li>Pérdida durante el entrenamiento</li>\n",
    "        <li>Evolución de epsilon a lo largo del entrenamiento</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHoR0XY71JTA"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DyLBp4p1JTB"
   },
   "source": [
    "### 3.5 Test del agente.\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.25 ptos):</strong> Cargar el modelo entrenado y ejecutar el agente entrenado durante 505 episodios consecutivos en diferentes periodos aleatorios desde el año 2015 hasta el 2024. Calcula la suma de recompensas por cada ejecución. Para conseguir este punto, ejecuta:\n",
    "    <ul>\n",
    "        <li>Un gráfico con la suma de las recompensas respecto de los episodios, incluyendo el umbral de recompensa establecido</li>\n",
    "        <li>Almacena la recompensa media obtenida en las 100 partidas en la variable <code>mean_reward_agentduelingDQN</code> y la última recompensa obtenida en el entrenamiento en <code>mean_reward_agentduelingDQN_last</code></li>\n",
    "    </ul>\n",
    "Además, realiza la siguiente análisis con el modelo para el entorno utilizado en el entrenamiento durante :\n",
    " <ul>\n",
    " <li>Reproducir una partida completa del agente entrenado y mostrar el resultado final, incluyendo el valor total del portafolio al final del episodio.</li>\n",
    "        <li>Generar un fichero CSV que registre los resultados de las interacciones del agente con el mercado en cada episodio y muestra por pantalla las últimas 30 acciones.</li>\n",
    " </ul>\n",
    "<strong>Comenta TODOs los resultados obtenidos en este apartado. ¿A qué conclusiones podemos llegar? ¿Cómo podríamos mejorar el entrenamiento y qué implicaciones tendría?</strong>\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQSAsiD81JTB"
   },
   "outputs": [],
   "source": [
    "file_path =  \"stock_trading_agent_ddqn.csv\"\n",
    "env = None #TODO\n",
    "\n",
    "\n",
    "\n",
    "mean_reward_agentduelingDQN = 0 #TODO\n",
    "mean_reward_agentduelingDQN_last = 0 #TODO\n",
    "print(f\"La recompensa media obtenida por el agente DQN en las 100 partidas de test es: {mean_reward_agentduelingDQN:.2f} puntos.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETW8_5TL1JTD"
   },
   "source": [
    "\n",
    "## 4.Comparación de los resultados (1.5 pto.)\n",
    "\n",
    "Ahora vamos a comparar los resultados, si has seguido todas las indicaciones, habrás almacenado métricas bastante interesantes que te permitirán interpretar los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wf3qjvYV1JTD"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define los datos de la tabla\n",
    "data = [\n",
    "    [\"DQN\", mean_reward_dqn_last, mean_reward_dqn, time_dqn],\n",
    "    [\"Dueling DQN\", mean_reward_agentduelingDQN_last, mean_reward_agentduelingDQN, time_ddqn],\n",
    "]\n",
    "\n",
    "# Define los encabezados de la tabla\n",
    "headers = [\"Agente\", \"Media Reward de Entrenamiento\", \"Media test con 100 Partidas Aleatorias\" , \"Tiempo entrenamiento.\"]\n",
    "\n",
    "# Imprime la tabla\n",
    "table = tabulate(data, headers, tablefmt=\"pipe\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zo0jZp-h1JTD"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (1.5 ptos):</strong>\n",
    "\n",
    "Comentar los resultados obtenidos. ¿Qué agente a obtenido mejor resultados? Justificalo.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsEGnkks1JTD"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentario:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cI1eaU4a1JTD"
   },
   "source": [
    "## 5. Compara los agentes en otro entorno.  (2 ptos)\n",
    "\n",
    "En esta parte de la PEC vamos a comparar cómo se desenvuelven dichos agentes en otro entorno diferente implementado por un tercero.\n",
    "\n",
    "Uno de los beneficios de haber utilizado Gymnasium es que podemos rápidamente utilizar nuestros algoritmos en todo aquel entorno que comparta la misma interfaz. Un ejemplo puede se [Cart Pole](https://gymnasium.farama.org/environments/classic_control/cart_pole/), un entorno clásico que consiste en un carrito sobre el que se apoya una barra vertical. El objetivo es mantener la barra en equilibrio evitando que caiga, aplicando pequeñas fuerzas al carrito hacia la derecha o hacia la izquierda. Las únicas acciones posibles son estas fuerzas, que permiten al algoritmo aprender a mantener el equilibrio de la barra en la posición correcta.\n",
    "\n",
    "![cartpole](https://gymnasium.farama.org/_images/cart_pole.gif)\n",
    "\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (2 ptos):</strong>\n",
    "\n",
    "Ejecuta el agente DQN y Dueling DQN en el nuevo entorno. Una vez lo hayar realizado, implementar una tabla como la mostrar en el ejercico anterior y análiza los resultados. ¿Contínua siendo el mismo agente el que mejor resultado ha obtenido?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7Tu93R01JTD"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Configuración de hiperparámetros para dqn.\n",
    "lr = None             # Velocidad de aprendizaje ajustada para mejor convergencia\n",
    "MEMORY_SIZE = None     # Capacidad de memoria reducida, suficiente para un entorno simple como CartPole\n",
    "MAX_EPISODES = 5000     # Número máximo de episodios reducido, ya que CartPole es un problema más sencillo\n",
    "EPSILON = None             # Valor inicial de epsilon (alta exploración inicial)\n",
    "EPSILON_DECAY = None    # Decaimiento de epsilon ajustado para un descenso más gradual\n",
    "GAMMA = None           # Factor de descuento gamma l\n",
    "BATCH_SIZE = None         # Tamaño del lote para el entrenamiento\n",
    "BURN_IN = None           # Episodios iniciales para llenar el buffer de experiencia antes de entrenar\n",
    "DNN_UPD = 1             # Frecuencia de actualización de la red neuronal (cada paso)\n",
    "DNN_SYNC = 1000         # Frecuencia de sincronización de pesos\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFeqn26fpm3g"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def test_model(ag, env):\n",
    "    all_rewards = []\n",
    "    # Usamos tqdm para el bucle de episodios\n",
    "    for i_episode in tqdm(range(100), desc=\"Progreso de episodios\"):\n",
    "        #rodo\n",
    "        pass\n",
    "\n",
    "    return all_rewards\n",
    "\n",
    "\n",
    "\n",
    "def plot_test(rewards, th):\n",
    "    #TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RANwmV4WpuM6"
   },
   "outputs": [],
   "source": [
    "#utiliza las funciones anteriores para imprimir la evolución de los agenetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMW_Jfvi1JTE"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Muestras los datos de los entrenamientos de cada agente.\n",
    "data = [\n",
    "    [\"DQN\", mean_reward_dqn_last, mean_reward_dqn, time_dqn],\n",
    "    [\"Dueling DQN\", mean_reward_agentduelingDQN_last, mean_reward_agentduelingDQN, time_ddqn],\n",
    "]\n",
    "\n",
    "# Define los encabezados de la tabla\n",
    "headers = [\"Agente\", \"Media Reward de Entrenamiento\", \"Media test con 100 Partidas Aleatorias\" , \"Tiempo entrenamiento.\"]\n",
    "\n",
    "# Imprime la tabla\n",
    "table = tabulate(data, headers, tablefmt=\"pipe\")\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
